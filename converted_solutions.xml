<?xml version="1.0" encoding="UTF-8" ?>

<appendix xml:id="appendix-exercise-solutions">
  <title>Exercise solutions</title>
  
  <introduction>
    <p>
      This appendix contains solutions to odd-numbered exercises from each chapter.
    </p>
  </introduction>
  
  <section xml:id="sec-exercise-solutions-01">
    <title>Chapter 1</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp01-exercises" />.
    </p>

    <solution xml:id="sol-ch01-ex01">
<p>23 observations and 7 variables.
</p>
    </solution>

    <solution xml:id="sol-ch01-ex03">
<p><ol marker="a.">
  <li><p>"Is there an association between air pollution exposure and preterm births?"</p></li>
  <li><p>Concentrations of carbon monoxide, nitrogen dioxide, ozone, and particulate matter with an aerodynamic diameter of 10 micrometres or less (PM<m>_{10}</m>) measured at air quality monitoring stations as well as length of gestation. Continuous numerical variables</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex05">
<p><ol marker="a.">
  <li><p>"What is the effect of gamification on learning outcomes compared to traditional teaching methods?"</p></li>
  <li><p>Gender (categorical), level of studies (categorical, ordinal), academic major (categorical), expertise in English language (categorical, ordinal), use of personal computers and games (categorical, ordinal), treatment group (categorical), score (numerical, discrete)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex07">
<p><ol marker="a.">
  <li><p>Treatment: <m>10/43 = 0.23 \rightarrow 23\%</m></p></li>
  <li><p>A higher percentage of patients in the treatment group were pain free 24 hours after receiving acupuncture</p></li>
  <li><p>Explanatory: acupuncture or not. Response: if the patient was pain free or not</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex09">
<p><ol marker="a.">
  <li><p>Experiment; researchers are evaluating the effect of fines on parents' behavior related to picking up their children late from daycare</p></li>
  <li><p>Number of late pickups (discrete numerical)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex11">
<p><ol marker="a.">
  <li><p>344 cases (penguins) are included in the data</p></li>
  <li><p>There are 3 categorical variables in the data: species (Adelie, Chinstrap, Gentoo), island (Torgersen, Biscoe, and Dream), and sex (female and male)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex13">
<p><ol marker="a.">
  <li><p>Airport ownership status (public/private), airport usage status (public/private), region (Central, Eastern, Great Lakes, New England, Northwest Mountain, Southern, Southwest, Western Pacific), latitude, and longitude</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex15">
<p><ol marker="a.">
  <li><p>Year, number of baby girls named Fiona born in that year, nation</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex17">
<p><ol marker="a.">
  <li><p>County, state, driver's race, whether the car was searched or not, and whether the driver was arrested or not</p></li>
  <li><p>Response: whether the car was searched or not. Explanatory: race of the driver</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch01-ex19">
<p><ol marker="a.">
  <li><p>Observational study</p></li>
  <li><p>Oliver and Lily</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-02">
    <title>Chapter 2</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp02-exercises" />.
    </p>

    <solution xml:id="sol-ch02-ex01">
<p><ol marker="a.">
  <li><p>Population mean, <m>\mu_{2007} = 52</m>; sample mean, <m>\bar{x}_{2008} = 58</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex03">
<p><ol marker="a.">
  <li><p>Population: all births, sample: 143,196 births between 1989 and 1993 in Southern California</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex05">
<p><ol marker="a.">
  <li><p>The population of interest is all college students studying statistics. The sample consists of 365 such students</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex07">
<p><ol marker="a.">
  <li><p>Observation</p></li>
  <li><p>Sample statistic (mean)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex09">
<p><ol marker="a.">
  <li><p>Observational</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex11">
<p><ol marker="a.">
  <li><p>Positive, non-linear, somewhat strong. Countries in which a higher percentage of the population have access to the internet also tend to have higher average life expectancies, however rise in life expectancy trails off before around 80 years old</p></li>
  <li><p>Wealth: countries with individuals who can widely afford the internet can probably also afford basic medical care. (Note: Answers may vary.)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex13">
<p><ol marker="a.">
  <li><p>Simple random sampling is okay. In fact, it's rare for simple random sampling to not be a reasonable sampling method!</p></li>
  <li><p>Students of similar ages are probably going to have more similar opinions, and we want clusters to be diverse with respect to the outcome of interest, so this would **not** be a good approach. (Additional thought: the clusters in this case may also have very different numbers of people, which can also create unexpected sample sizes.)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex15">
<p><ol marker="a.">
  <li><p>The cases are 200 randomly sampled men and women</p></li>
  <li><p>The explanatory variable is dispositional attitude</p></li>
  <li><p>This is an observational study since there is no random assignment to treatments</p></li>
  <li><p>Yes, the results of the study can be generalized to the population at large since the sample is random</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex17">
<p><ol marker="a.">
  <li><p>Simple random sample. Non-response bias, if only those people who have strong opinions about the survey responds their sample may not be representative of the population</p></li>
  <li><p>Convenience sample. This will have a similar issues to handing out surveys to friends</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex19">
<p><ol marker="a.">
  <li><p>Exam performance</p></li>
  <li><p>Wearing glasses or not</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex21">
<p><ol marker="a.">
  <li><p>Experiment</p></li>
  <li><p>Since the researchers want to ensure equal representation of those wearing glasses and not wearing glasses, wearing glasses is a blocking variable</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex23">
<p>Need randomization and blinding. One possible outline: (1) Prepare two cups for each participant, one containing regular Coke and the other containing Diet Coke. Make sure the cups ar identical and contain equal amounts of soda. Label the cups (regular) and B (diet). (Be sure to randomize A and B for each trial!) (2) Give each participant the two cups, one cup at a time, in random order, and ask the participant to record a value that indicates ho much she liked the beverage. Be sure that neither the participant nor the person handing out the cups knows the identity of th beverage to make this a double-blind experiment. (Answers may vary.)
</p>
    </solution>

    <solution xml:id="sol-ch02-ex25">
<p><ol marker="a.">
  <li><p>Experiment</p></li>
  <li><p>Yes, gender</p></li>
  <li><p>Since this is an experiment, we can make a causal statement. However, since the sample is not random, the causal statement cannot be generalized to the population at large</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex27">
<p><ol marker="a.">
  <li><p>Non-responders may have a different response to this question, e.g., parents who returned the surveys likely don't have difficulty spending time with their children</p></li>
  <li><p>There is no control group in this study, this is an observational study, and there may be confounding variables, e.g., these people may go running because they are generally healthier and/or do other exercises</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch02-ex29">
<p><ol marker="a.">
  <li><p>Randomized controlled experiment</p></li>
  <li><p>No, because the participants were volunteers</p></li>
  <li><p>The statement should say "evidence" instead of "proof"</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-03">
    <title>Chapter 3</title>
    <p>
      Application chapter, no exercises.
    </p>
  </section>
  <section xml:id="sec-exercise-solutions-04">
    <title>Chapter 4</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp04-exercises" />.
    </p>

    <solution xml:id="sol-ch04-ex01">
<p><ol marker="a.">
  <li><p>We see the order of the categories and the relative frequencies in the bar plot</p></li>
  <li><p>We usually prefer to use a bar plot as we can also see the relative frequencies of the categories in this graph</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch04-ex03">
<p><ol marker="a.">
  <li><p>The horizontal locations at which the age groups break into the various opinion levels differ, which indicates that likelihood of supporting protests varies by age group. Two variables may be associated</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch04-ex05">
<p><ol marker="a.">
  <li><p>Number of participants in each group</p></li>
  <li><p>The standardized bar plot should be displayed as a way to visualize the survival improvement in the treatment versus the control group</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch04-ex07">
<p><ol marker="a.">
  <li><p>The ridge plots do not tell us about the relationship between meat consumption and life expectancy. While it is true that the high income group of countries has highest meat consumption and highest life expectancy, we can't, for example, differentiate meat consumption across the low and middle income groups (so as to connect to life expectancy). Additionally, we don't know anything about the relationship betwen meat consumption and life expectancy *within* an income group</p></li>
  <li><p>In order to investigate a specific confounding variable, first break the data into categories according to that confounding variable (here, income). Then look at the relationship of interest (here meat consumption and life expectancy) separately for each of the levels of the confounding variable (income)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch04-ex09">
<p><ol marker="a.">
  <li><p>41% of the JetBlue flights are delayed. 40.7% of the United Airlines flights are delayed</p></li>
  <li><p>Note that JetBlue had substantially more flights than United out of BQN (where there was a high delay percentage). United had substantially more flights than United out of SFO and LAX, both of which had low delay percentages. So JetBlue's overall percentage delay is bumped up due to the BQN flights, and United's overall percentage delay is bumped down due to the SFO and LAX flights</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-05">
    <title>Chapter 5</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp05-exercises" />.
    </p>

    <solution xml:id="sol-ch05-ex01">
<p><ol marker="a.">
  <li><p>Positive association: mammals with longer gestation periods tend to live longer as well</p></li>
  <li><p>No, they are not independent. See part (a)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch05-ex03">
<p>The graph below shows a ramp up period. There may also be a period of exponential growth at the start before the size of the petri dish becomes a factor in slowing growth.

    ```{r}
    #| out-width: 30%
    set.seed(2406)
    par(mar = c(1.5, 1.5, 0.5, 0.5), mgp = c(0.3, 0.7, 0),  mfrow = c(1,1), cex.lab = 1.5)
    curve(-1*dexp(x, rate = 4), lwd = 2, xlab = "time", ylab = "number of bacteria cells", axes = FALSE)
    box()
    ```

    </p>
    </solution>

    <solution xml:id="sol-ch05-ex05">
<p><ol marker="a.">
  <li><p>Decrease: the new score is smaller than the mean of the 24 previous scores</p></li>
  <li><p>The new score is more than 1 standard deviation away from the previous mean, so increase</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch05-ex07">
<p>Any 10 employees whose average number of days off is between the minimum and the mean number of days off for the entire workforce at this plant.

    </p>
    </solution>

    <solution xml:id="sol-ch05-ex09">
<p><ol marker="a.">
  <li><p>Dist B has a higher mean since <m>20 &gt; 13</m>, and a higher standard deviation since 20 is further from the rest of the data than 13</p></li>
  <li><p>Dist B has a higher mean since all values in this Dist Are higher than those in Dist A, but both distribution have the same standard deviation since they are equally variable around their respective means</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch05-ex11">
<p><ol marker="a.">
  <li><p>About 26</p></li>
  <li><p>Q1: between 15 and 20, Q3: between 35 and 40, IQR: about 20</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch05-ex13">
<p>The histogram shows that the distribution is bimodal, which is not apparent in the box plot. The box plot makes it easy to identify more precise values of observations outside of the whiskers.

    </p>
    </solution>

    <solution xml:id="sol-ch05-ex15">
<p><ol marker="a.">
  <li><p>Right skewed, there is a natural boundary at 0 and only a few people have many pets. Center: median, variability: IQR</p></li>
  <li><p>Symmetric. Center: mean, variability: standard deviation</p></li>
  <li><p>Left skewed. Center: median, variability: IQR</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch05-ex17">
<p>No, we would expect this distribution to be right skewed. There are two reasons for this: there is a natural boundary at 0 (it is not possible to watch less than 0 hours of TV) and the standard deviation of the distribution is very large compared to the mean.

    </p>
    </solution>

    <solution xml:id="sol-ch05-ex19">
<p>No, the outliers are likely the maximum and the minimum of the distribution so a statistic based on these values cannot be robust to outliers.

    </p>
    </solution>

    <solution xml:id="sol-ch05-ex21">
<p>The 75th percentile is 82.5, so 5 students will get an A. Also, by definition 25% of students will be above the 75th percentile.

    </p>
    </solution>

    <solution xml:id="sol-ch05-ex23">
<p><ol marker="a.">
  <li><p>If <m>\frac{\bar{x}}{median} = 1</m>, then <m>\bar{x} = median</m>. This is most likely to be the case for symmetric distributions</p></li>
  <li><p>If <m>\frac{\bar{x}}{median} &gt; 1</m>, then <m>\bar{x} &gt; median</m>. This is most likely to be the case for right skewed distributions, since the mean is affected (and pulled up) by the higher values more so than the median</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch05-ex25">
<p><ol marker="a.">
  <li><p>The distribution of percentage of population that is Hispanic is extremely right skewed with majority of counties with less than 10% Hispanic residents. However there are a few counties that have more than 90% Hispanic population. It might be preferable to, in certain analyses, to use the log-transformed values since this distribution is much less skewed</p></li>
  <li><p>Both visualizations are useful, but if we could only examine one, we should examine the map since it explicitly ties geographic data to each county's percentage</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-06">
    <title>Chapter 6</title>
    <p>
      Application chapter, no exercises.
    </p>
  </section>
  <section xml:id="sec-exercise-solutions-07">
    <title>Chapter 7</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp07-exercises" />.
    </p>

    <solution xml:id="sol-ch07-ex01">
<p><ol marker="a.">
  <li><p>The residual plot will show randomly distributed residuals around 0. The variance is also approximately constant</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex03">
<p><ol marker="a.">
  <li><p>Strong relationship, but a straight line would not fit the data</p></li>
  <li><p>Weak relationship, and trying a linear fit would be reasonable</p></li>
  <li><p>Strong relationship, and a linear fit would be reasonable</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex05">
<p><ol marker="a.">
  <li><p>Exam 2 since there is less of a scatter in the plot of course grade versus exam 2. Notice that the relationship between Exam 1 and the course grade appears to be slightly nonlinear</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex07">
<p><ol marker="a.">
  <li><p><m>r = -0.7</m> <m>\rightarrow</m> (4)</p></li>
  <li><p><m>r = 0.06</m> <m>\rightarrow</m> (1)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex09">
<p><ol marker="a.">
  <li><p>There is a moderate, positive, and linear relationship between shoulder girth and height</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex11">
<p><ol marker="a.">
  <li><p>There is a somewhat weak, positive, possibly linear relationship between the distance traveled and travel time. There is clustering near the lower left corner that we should take special note of</p></li>
  <li><p>Changing units doesn't affect correlation: <m>r = 0.636</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex13">
<p>we can write the amount of meat consumption as an exact linear function of the amount of carbohydrate consumption. (a) <m>carbs = meat - 3.</m> (b) <m>carbs = meat + 2.</m> (c) <m>carbs = 2 \times meat.</m> Since the slopes are positive and these are perfect linear relationships, the correlation will be exactly 1 in all three parts. An alternative way to gain insight into this solution is to create a mock dataset, e.g., 5 countries with meat consumption of 10, 20, 50, 75, and 100 kg per capita, find the related carbohydrate consumption for each mock country, then create a scatterplot.
</p>
    </solution>

    <solution xml:id="sol-ch07-ex15">
<p>Correlation: no units. Intercept: cal. Slope: cal/cm.
</p>
    </solution>

    <solution xml:id="sol-ch07-ex17">
<p>Over-estimate. Since the residual is calculated as <m>observed - predicted</m>, a negative residual means that the predicted value is higher than the observed value.
</p>
    </solution>

    <solution xml:id="sol-ch07-ex19">
<p><ol marker="a.">
  <li><p>There is a positive, moderate, linear association between number of calories and amount of carbohydrates. In addition, the amount of carbohydrates is more variable for menu items with higher calories, indicating non-constant variance. There also appear to be two clusters of data: a patch of about a dozen observations in the lower left and a larger patch on the right side</p></li>
  <li><p>With a regression line, we can predict the amount of carbohydrates for a given number of calories. This may be useful if only calorie counts for the food items are posted but the amount of carbohydrates in each food item is not readily available</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex21">
<p><ol marker="a.">
  <li><p>First calculate the slope: <m>b_1 = R\times s_y/s_x = 0.636 \times 113 / 99 = 0.726</m>. Next, make use of the fact that the regression line passes through the point <m>(\bar{x},\bar{y})</m>: <m>\bar{y} = b_0 + b_1 \times \bar{x}</m>. Plug in <m>\bar{x}</m>, <m>\bar{y}</m>, and <m>b_1</m>, and solve for <m>b_0</m>: 51. Solution: <m>\widehat{travel~time} = 51 + 0.726 \times distance</m></p></li>
  <li><p><m>R^2 = 0.636^2 = 0.40</m>. About 40% of the variability in travel time is accounted for by the model, i.e., explained by the distance travelled</p></li>
  <li><p><m>e_i = y_i - \hat{y}_i = 168 - 126 = 42</m> minutes. A positive residual means that the model underestimates the travel time</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex23">
<p><ol marker="a.">
  <li><p><m>\widehat{\texttt{poverty}} = 4.60 + 2.05 \times \texttt{unemployment\_rate}.</m></p></li>
  <li><p>For each additional percentage increase in unemployment rate, poverty rate is predicted to be higher, on average, by 2.05\%</p></li>
  <li><p><m>\sqrt{0.46} = 0.678.</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex25">
<p><ol marker="a.">
  <li><p>There is an outlier in the bottom right. Since it is far from the center of the data, it is a point with high leverage. It is also an influential point since, without that observation, the regression line would have a very different slope</p></li>
  <li><p>The observation is in the center of the data (in the x-axis direction), so this point does *not* have high leverage. This means the point won't have much effect on the slope of the line and so is not an influential point</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex27">
<p><ol marker="a.">
  <li><p>There is a negative, moderate-to-strong, somewhat linear relationship between percent of families who own their home and the percent of the population living in urban areas in 2010. There is one outlier: a state where 100% of the population is urban. The variability in the percent of homeownership also increases as we move from left to right in the plot</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex29">
<p><ol marker="a.">
  <li><p>True</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch07-ex31">
<p><ol marker="a.">
  <li><p><m>r = 0.7 \to (1)</m></p></li>
  <li><p><m>r = -0.91 \to (2)</m></p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-08">
    <title>Chapter 8</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp08-exercises" />.
    </p>

    <solution xml:id="sol-ch08-ex01">
<p>Annika is right. All variables being highly correlated, including the predictor variables being highly correlated with each other, is not desirable as this would result in multicollinearity.
</p>
    </solution>

    <solution xml:id="sol-ch08-ex03">
<p><ol marker="a.">
  <li><p>The association between meat consumption and life expectancy is positive, moderate, and curved</p></li>
  <li><p>Within an income bracket, the relationship between meat consumption and life expectancy is not nearly as strong (as compared to when the data are aggregated into one plot)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch08-ex05">
<p>No, they shouldn't include all variables as `days_since_start` and `days_since_race` are perfectly correlated with each other. They should only include one of them.
</p>
    </solution>

    <solution xml:id="sol-ch08-ex07">
<p><ol marker="a.">
  <li><p><m>\widehat{\texttt{weight}} = 7.270 - 0.593 \times \texttt{habit}_\texttt{smoker}</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch08-ex09">
<p><ol marker="a.">
  <li><p>Horror movies</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch08-ex11">
<p><ol marker="a.">
  <li><p><m>\widehat{\texttt{weight}} = -3.82 + 0.26 \times \texttt{weeks} + 0.02 \times \texttt{mage} + 0.37 \times \texttt{sex}_\texttt{male} + 0.02 \times \texttt{visits} - 0.43 \times \texttt{habit}_\texttt{smoker}.</m></p></li>
  <li><p>Habit might be correlated with one of the other variables in the model, which introduces multicollinearity and complicates model estimation</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch08-ex13">
<p>Remove `gained`.
</p>
    </solution>

    <solution xml:id="sol-ch08-ex15">
<p>Add `weeks`.
</p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-09">
    <title>Chapter 9</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp09-exercises" />.
    </p>

    <solution xml:id="sol-ch09-ex01">
<p><ol marker="a.">
  <li><p>False. The line is fit to predict the probability of success, not the binary outcome</p></li>
  <li><p>True</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch09-ex03">
<p><ol marker="a.">
  <li><p>There are a few potential outliers, e.g., on the left in the variable total length, but nothing that will be of serious concern in a dataset this large</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch09-ex05">
<p><ol marker="a.">
  <li><p>The logistic model relating <m>\hat{p}</m> to the predictors may be written as <m>\log\left( \frac{\hat{p}}{1 - \hat{p}} \right) = 33.5095 - 1.4207\times \texttt{sex}_{\texttt{male}} - 0.2787 \times \texttt{skull\_w} + 0.5687 \times \texttt{total\_l} - 1.8057 \times \texttt{tail\_l}.</m> Only `total_l` has a positive association with a possum being from Victoria</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch09-ex07">
<p><ol marker="a.">
  <li><p>The variable `exclaim_subj` should be removed, since it's removal reduces AIC the most (and the resulting model has lower AIC than the None Dropped model)</p></li>
  <li><p>Removing any variable will increase AIC, so we should not remove any variables from this set</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch09-ex09">
<p><ol marker="a.">
  <li><p>The AIC is smallest using the variables `sex`, `head_l`, `skull_w`, `total_l`, and `tail_l` to predict region (AIC = 83.52), so we would choose that model</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-10">
    <title>Chapter 10</title>
    <p>
      Application chapter, no exercises.
    </p>
  </section>
  <section xml:id="sec-exercise-solutions-11">
    <title>Chapter 11</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp11-exercises" />.
    </p>

    <solution xml:id="sol-ch11-ex01">
<p><ol marker="a.">
  <li><p>Mean. Each student reports a numerical value: a number of hours</p></li>
  <li><p>Proportion. Each student reports Yes or No, so this is a categorical variable and we use a proportion</p></li>
  <li><p>Proportion. Each student reports whether s/he expects to get a job, so this is a categorical variable and we use a proportion</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch11-ex03">
<p><ol marker="a.">
  <li><p>Alternative</p></li>
  <li><p>Alternative</p></li>
  <li><p>Null</p></li>
  <li><p>Null</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch11-ex05">
<p><ol marker="a.">
  <li><p><m>H_0: \mu = 8</m> (On average, New Yorkers sleep 8 hours a night.) <m>H_A: \mu &lt; 8</m> (On average, New Yorkers sleep less than 8 hours a night.)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch11-ex07">
<p><ol marker="a.">
  <li><p>Proportion of all patients who had cardiovascular problems: <m>\frac{7,979}{227,571} \approx 0.035</m></p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-12">
    <title>Chapter 12</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp12-exercises" />.
    </p>

    <solution xml:id="sol-ch12-ex01">
<p><ol marker="a.">
  <li><p>The statistic is the sample proportion (0.289); the parameter is the population proportion (unknown)</p></li>
  <li><p>Bootstrap sample proportion</p></li>
  <li><p>Roughly (0.22, 0.35)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch12-ex03">
<p>With 98% confidence, the true proportion of all US adults (in 2022) who get news from social media sometimes or often is between 0.487 and 0.51.
</p>
    </solution>

    <solution xml:id="sol-ch12-ex05">
<p><ol marker="a.">
  <li><p>A or perhaps D</p></li>
  <li><p>B or C</p></li>
  <li><p>None</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch12-ex07">
<p><ol marker="a.">
  <li><p>This claim is reasonable, since the entire interval lies above 50%</p></li>
  <li><p>A 90% confidence interval will be narrower than a 95% confidence interval. Even without calculating the interval, we can tell that 70% would not fall in the interval, and we would reject the researcher's conjecture based on a 90% confidence level as well</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-13">
    <title>Chapter 13</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp13-exercises" />.
    </p>

    <solution xml:id="sol-ch13-ex01">
<p><ol marker="a.">
  <li><p>[computed value]</p></li>
  <li><p>[computed value]</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch13-ex03">
<p><ol marker="a.">
  <li><p>Verbal: <m>N(\mu = 151, \sigma = 7)</m>, Quant: <m>N(\mu = 153, \sigma = 7.67)</m></p></li>
  <li><p>She scored 1.29 standard deviations above the mean on the Verbal  Reasoning section and 0.52 standard deviations above the mean on the Quantitative Reasoning section</p></li>
  <li><p><m>100\% - 90\% = 10\%</m> did better than her on VR, and <m>100\% - 70\% = 30\%</m> did better than her on QR</p></li>
  <li><p>Answer to part</p></li>
  <li><p>since we cannot use the normal probability table to calculate probabilities and percentiles without a normal model</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch13-ex05">
<p><ol marker="a.">
  <li><p><m>Z = 0.84</m>, which corresponds to approximately 159 on QR</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch13-ex07">
<p><ol marker="a.">
  <li><p><m>Z = 1.2</m>, <m>P(Z &gt; 1.2) = 0.1151</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch13-ex09">
<p><ol marker="a.">
  <li><p><m>N(25, 2.78)</m></p></li>
  <li><p>Since <m>IQR = Q3 - Q1</m>, we first need to find <m>Q3</m> and <m>Q1</m> and take the difference between the two. Remember that <m>Q3</m> is the <m>75^{th}</m> and <m>Q1</m> is the <m>25^{th}</m> percentile of a distribution. Q1 = 23.13, Q3 = 26.86, IQR = 26. 86 - 23.13 = 3.73</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch13-ex11">
<p><ol marker="a.">
  <li><p>Recall that the general formula is <m>point~estimate \pm z^{\star} \times SE</m>. First, identify the three different values. The point estimate is 45%, <m>z^{\star} = 1.96</m> for a 95% confidence level, and <m>SE = 1.2\%</m>. Then, plug the values into the formula: <m>45\% \pm 1.96 \times 1.2\% \quad\to\quad (42.6\%, 47.4\%)</m> We are 95% confident that the proportion of US adults who live with one or more chronic conditions is between 42.6% and 47.4%</p></li>
  <li><p>False. Confidence intervals provide a range of plausible values, and sometimes the truth is missed. A 95% confidence interval "misses" about 5% of the time. (ii) True. Notice that the description focuses on the true population value. (iii) True. If we examine the 95% confidence interval, we can see that 50% is not included in this interval. This means that in a hypothesis test, we would reject the null hypothesis that the proportion is 0.5. (iv) False. The standard error describes the uncertainty in the overall estimate from natural fluctuations due to randomness, not the uncertainty corresponding to individuals' responses</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch13-ex13">
<p>A Z score of 0.47 denotes that the sample proportion is 0.47 standard errors greater than the hypothesized value of the population proportion. 
</p>
    </solution>

    <solution xml:id="sol-ch13-ex15">
<p><ol marker="a.">
  <li><p>Sampling distribution</p></li>
  <li><p>Standard error</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-14">
    <title>Chapter 14</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp14-exercises" />.
    </p>

    <solution xml:id="sol-ch14-ex01">
<p><ol marker="a.">
  <li><p><m>H_0</m>: Anti-depressants do not affect the symptoms of Fibromyalgia. <m>H_A</m>: Anti-depressants do affect the symptoms of Fibromyalgia (either helping or harming)</p></li>
  <li><p>Concluding that anti-depressants do not affect Fibromyalgia symptoms when they actually do</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch14-ex03">
<p><ol marker="a.">
  <li><p>Scenario</p></li>
  <li><p>Scenario</p></li>
  <li><p>They are equal. The sample size does not affect the calculation of the p-value for a given Z score</p></li>
  <li><p>is higher. If the null hypothesis is harder to reject (lower <m>\alpha</m>), then we are more likely to make a Type II error when the alternative hypothesis is true</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch14-ex05">
<p>The hypotheses should be about the population proportion (<m>p</m>), not the sample proportion. The null hypothesis should have an equal sign. The alternative hypothesis should have a not-equals sign, and it should reference the null value, <m>p_0 = 0.6</m>, not the observed sample proportion. The correct way to set up these hypotheses is: <m>H_0: p = 0.6</m> and <m>H_A: p \neq 0.6</m>.
</p>
    </solution>

    <solution xml:id="sol-ch14-ex07">
<p>Regardless of whether the students were making 95% intervals or 90% intervals, seven students with intervals that miss <m>\pi</m> seems totally reasonable.  The students should not be docked for the intervals missing <m>\pi</m>.
</p>
    </solution>

    <solution xml:id="sol-ch14-ex09">
<p>True. If the sample size gets ever larger, then the standard error will become ever smaller. Eventually, when the sample size is large enough and the standard error is tiny, we can find statistically discernible yet very small differences between the null value and point estimate (assuming they are not exactly equal).
</p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-15">
    <title>Chapter 15</title>
    <p>
      Application chapter, no exercises.
    </p>
  </section>
  <section xml:id="sec-exercise-solutions-16">
    <title>Chapter 16</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp16-exercises" />.
    </p>

    <solution xml:id="sol-ch16-ex01">
<p>First, the hypotheses should be about the population proportion (<m>p</m>), not the sample proportion. Second, the null value should be what we are testing (0.25), not the observed value (0.29). The correct way to set up these hypotheses is: <m>H_0: p = 0.25</m> and <m>H_A: p &gt; 0.25.</m>
</p>
    </solution>

    <solution xml:id="sol-ch16-ex03">
<p><ol marker="a.">
  <li><p><m>H_0 : p = 0.20,</m>  <m>H_A : p &gt; 0.20.</m></p></li>
  <li><p>Answers will vary. Each student can be represented with a card. Take 100 cards, 20 black cards representing those who support proposals to defund police departments and 80 red cards representing those who do not. Shuffle the cards and draw with replacement (shuffling each time in between draws) 650 cards representing the 650 respondents to the poll. Calculate the proportion of black cards in this sample, <m>\hat{p}_{sim},</m> i.e., the proportion of those who upport proposals to defund police departments. The p-value will be the proportion of simulations where <m>\hat{p}_{sim} \geq 0.245.</m> (Note: We would generally use a computer to perform the simulations.)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex05">
<p><ol marker="a.">
  <li><p><m>H_0: p = 0.5</m>, <m>H_A: p \ne 0.5</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex07">
<p><ol marker="a.">
  <li><p><m>SE(\hat{p}) = 0.189</m></p></li>
  <li><p>Yes</p></li>
  <li><p>The draws from the null hypothesis are discrete (only a few distinct options) and the mathematical model is continuous (infinite options on a continuum)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex09">
<p><ol marker="a.">
  <li><p>The null hypothesis simulation was done with <m>p=0.7</m>, and the data bootstrap simulation was done with <m>p = 0.6.</m></p></li>
  <li><p>The standard error of the sample proportion is given to be roughly 0.1 for both histograms</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex11">
<p><ol marker="a.">
  <li><p>The null hypothesis simulation distribution for testing. The data bootstrap distribution for confidence intervals</p></li>
  <li><p>We are 98% confident that the true proportion of all full-time student statistics majors who work at least 5 hours per week is between 35% and 80%</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex13">
<p><ol marker="a.">
  <li><p>False. Doesn't satisfy success-failure condition</p></li>
  <li><p>False. <m>SE_{\hat{p}} = 0.0243</m>, and <m>\hat{p} = 0.12</m> is only <m>\frac{0.12 - 0.08}{0.0243} = 1.65</m> SEs away from the mean, which would not be considered unusual</p></li>
  <li><p>False. Decreases the SE by a factor of <m>1/\sqrt{2}</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex15">
<p><ol marker="a.">
  <li><p>True. See the reasoning of 6.1(b)</p></li>
  <li><p>True. The independence and success-failure conditions are satisfied</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex17">
<p><ol marker="a.">
  <li><p>False. A confidence interval is constructed to estimate the population proportion, not the sample proportion</p></li>
  <li><p>True. By the definition of the confidence level</p></li>
  <li><p>True. The 95% CI is entirely above 50%</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex19">
<p>With a random sample, independence is satisfied. The success-failure condition is also satisfied. <m>ME = z^{\star} \sqrt{ \frac{\hat{p} (1-\hat{p})} {n} } = 1.96 \sqrt{ \frac{0.56 \times  0.44}{600} }= 0.0397 \approx 4\%.</m>
</p>
    </solution>

    <solution xml:id="sol-ch16-ex21">
<p><ol marker="a.">
  <li><p>No. The sample only represents students who took the SAT, and this was also an online survey</p></li>
  <li><p>90% of such random samples would produce a 90% confidence interval that includes the true proportion</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex23">
<p><ol marker="a.">
  <li><p>We want to check for a majority (or minority), so we use the following hypotheses: <m>H_0: p = 0.5</m> and <m>H_A: p \neq 0.5</m>. We have a sample proportion of <m>\hat{p} = 0.55</m> and a sample size of <m>n = 617</m> independents. Since this is a random sample, independence is satisfied. The success-failure condition is also satisfied: <m>617 \times 0.5</m> and <m>617 \times (1 - 0.5)</m> are both at least 10 (we use the null proportion <m>p_0 = 0.5</m> for this check in a one-proportion hypothesis test). Therefore, we can model <m>\hat{p}</m> using a normal distribution with a standard error of <m>SE = \sqrt{\frac{p(1 - p)}{n}} = 0.02</m>. (We use the null proportion <m>p_0 = 0.5</m> to compute the standard error for a one-proportion hypothesis test.) Next, we compute the test statistic: <m>Z = \frac{0.55 - 0.5}{0.02} = 2.5.</m> This yields a one-tail area of 0.0062, and a p-value of <m>2 \times 0.0062 = 0.0124.</m> Because the p-value is smaller than 0.05, we reject the null hypothesis. We have strong evidence that the support is different from 0.5, and since the data provide a point estimate above 0.5, we have strong evidence to support this claim by the TV pundit</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex25">
<p><ol marker="a.">
  <li><p><m>H_0: p = 0.5</m>. <m>H_A: p &gt; 0.5</m>. Independence (random sample, <m>&lt;10\%</m> of population) is satisfied, as is the success-failure conditions (using <m>p_0 = 0.5</m>, we expect 40 successes and 40 failures). <m>Z = 2.91</m> <m>\to</m> p- value <m>= 0.0018</m>. Since the p-value <m>&lt; 0.05</m>, we reject the null hypothesis. The data provide strong evidence that the rate of correctly identifying a soda for these people is discernibly better than just by random guessing</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex27">
<p><ol marker="a.">
  <li><p>The sample is from all computer chips manufactured at the factory during the week of production. We might be tempted to generalize the population to represent all weeks, but we should exercise caution here since the rate of defects may change over time</p></li>
  <li><p>Estimate the parameter using the data: <m>\hat{p} = \frac{27}{212} = 0.127</m></p></li>
  <li><p>Compute the <m>SE</m> using <m>\hat{p} = 0.127</m> in place of <m>p</m>: <m>SE \approx \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} = \sqrt{\frac{0.127(1 - 0.127)}{212}} = 0.023</m></p></li>
  <li><p>Recomputed standard error using <m>p = 0.1</m>: <m>SE = \sqrt{\frac{0.1(1 - 0.1)}{212}} = 0.021</m>. This value isn't very different, which is typical when the standard error is computed using relatively similar proportions (and even sometimes when those proportions are quite different!)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch16-ex29">
<p><ol marker="a.">
  <li><p>The visitors are from a simple random sample, so independence is satisfied. The success-failure condition is also satisfied, with both 64 and <m>752 - 64 = 688</m> above 10. Therefore, we can use a normal distribution to model <m>\hat{p}</m> and construct a confidence interval</p></li>
  <li><p>For a 90% confidence interval, use <m>z^{\star} = 1.65</m>. The confidence interval is <m>0.085 \pm 1.65 \times 0.010 \to (0.0685, 0.1015)</m>. We are 90% confident that 6.85% to 10.15% of first-time site visitors will register using the new design</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-17">
    <title>Chapter 17</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp17-exercises" />.
    </p>

    <solution xml:id="sol-ch17-ex01">
<p><ol marker="a.">
  <li><p>The parameter is <m>p_{Asican-Indian} - p_{Chinese}.</m>  The statistic is <m>\hat{p}_{Asian-Indian} - \hat{p}_{Chinese} = 223/4373 - 279/4736 = -0.008</m></p></li>
  <li><p><m>H_0: p_{Asian-Indian} - p_{Chinese} = 0;</m>, <m>H_A: p_{Asian-Indian} - p_{Chinese} \ne 0.</m>  The evidence is borderline but worth further study. There is not strong evidence that the true difference in proportion of current smokers is different across the two ethnic groups</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex03">
<p><ol marker="a.">
  <li><p>Roughly 0.00625</p></li>
  <li><p>We are 95% confident that the true proportion of Filipino Americans who are current smokers is between 5.2 and 7.7 percentage points higher in the control vaccine group than the proportion of Chinese Americans who smoke</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex05">
<p><ol marker="a.">
  <li><p>While the standard errors of the difference in proportion across the two graphs are roughly the same (approximately 0.012), the centers are not. Computational method A is centered at 0.07 (the difference in the observed sample proportions) and Computational method B is centered at 0</p></li>
  <li><p>Is the proportion of Bachelor's students who believe that their ability to complete the degree will be negatively impacted by the COVID-19 pandemic different than that of Associate's students?</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex07">
<p><ol marker="a.">
  <li><p>26 Yes and 94 No in Nevaripine and 10 Yes and 110 No in Lopinavir group</p></li>
  <li><p>Random assignment was used, so the observations in each group are independent. If the patients in the study are representative of those in the general population (something impossible to check with the given information), then we can also confidently generalize the findings to the population. The success-failure condition, which we would check using the pooled proportion (<m>\hat{p}_{pool} = 36/240 = 0.15</m>), is satisfied. <m>Z = 2.89</m> <m>\to</m> p-value <m>=0.0039</m>. Since the p-value is low, we reject <m>H_0</m>. There is strong evidence of a difference in virologic failure rates between the Nevaripine and Lopinavir groups. Treatment and virologic failure do not appear to be independent</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex09">
<p><ol marker="a.">
  <li><p>Standard error: <m>SE = \sqrt{\frac{0.79(1 - 0.79)}{347} + \frac{0.55(1 - 0.55)}{617}} = 0.03.</m> Using <m>z^{\star} = 1.96</m>, we get: <m>0.79 - 0.55 \pm 1.96 \times 0.03 \to (0.181, 0.299).</m> We are 95% confident that the proportion of Democrats who support the plan is 18.1% to 29.9% higher than the proportion of Independents who support the plan</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex11">
<p><ol marker="a.">
  <li><p>In effect, we're checking whether men are paid more than women (or vice-versa), and we'd expect these outcomes with either chance under the null hypothesis: <m>H_0: p = 0.5</m> and <m>H_A: p \neq 0.5.</m> We'll use <m>p</m> to represent the fraction of cases where men are paid more than women</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex13">
<p>Before we can calculate a confidence interval, we must first check that the conditions are met. There aren't at least 10 successes and 10 failures in each of the four groups (treatment/control and yawn/not yawn), <m>(\hat{p}_C - \hat{p}_T)</m> is not expected to be approximately normal and therefore cannot calculate a confidence interval for the difference between the proportions of participants who yawned in the treatment and control groups using large sample techniques and a critical Z score.
</p>
    </solution>

    <solution xml:id="sol-ch17-ex15">
<p><ol marker="a.">
  <li><p>False. The confidence interval includes 0</p></li>
  <li><p>False. As the confidence level decreases the width of the confidence level decreases as well</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex17">
<p><ol marker="a.">
  <li><p>Type I</p></li>
  <li><p>Type II</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch17-ex19">
<p>No. The samples at the beginning and at the end of the semester are not independent since the survey is conducted on the same students.
</p>
    </solution>

    <solution xml:id="sol-ch17-ex21">
<p><ol marker="a.">
  <li><p>The proportion of the normal curve centered at -0.1 with a standard deviation of 0.15 that is less than -2 * standard error is [computed value]</p></li>
  <li><p>The proportion of the normal curve centered at -0.1 with a standard deviation of 0.0671 that is less than 2 * standard error is [computed value]</p></li>
  <li><p>The larger the value of <m>\delta</m> and the larger the sample size, the more likely that the future study will lead to sample proportions which are able to reject the null hypothesis</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-18">
    <title>Chapter 18</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp18-exercises" />.
    </p>

    <solution xml:id="sol-ch18-ex01">
<p><ol marker="a.">
  <li><p>Two-way table is shown below. (b-i) <m>E_{row_1, col_1} = \frac{(row~1~total)\times(col~1~total)}{table~total} = 35</m>. This is lower than the observed value. (b-ii) <m>E_{row_2, col_2} = \frac{(row~2~total)\times(col~2~total)}{table~total} = 115</m>. This is lower than the observed value.

    ```{r}
    tribble(
       ~Treatment, ~Yes, ~No, ~Total,
       "Patch + support group", 40, 110, 150,
       "Only patch", 30, 120, 150,
       "Total", 70, 230, 300
    ) |>
       kbl(linesep = "", booktabs = TRUE, align = "lrrr") |>
       kable_styling(bootstrap_options = c("striped", "condensed"), 
                     latex_options = "HOLD_position", 
                     full_width = FALSE) |>
       column_spec(1, width = "10em") |>
       column_spec(2:4, width = "5em") |>
       add_header_above(c(" " = 1, "Quit" = 2, " " = 1))
    ```</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch18-ex03">
<p><ol marker="a.">
  <li><p>Sun = 0.343, Partial = 0.325, Shade = 0.331</p></li>
  <li><p>Yes</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch18-ex05">
<p>The original dataset will have a higher Chi-squared statistic than the randomized dataset.

    </p>
    </solution>

    <solution xml:id="sol-ch18-ex07">
<p><ol marker="a.">
  <li><p>The two variables are independent</p></li>
  <li><p>The null hypothesis is that the variables are independent; the alternative hypothesis is that the variables are associated. The p-value is extremely small. The habitat provides information about the likelihood of being in the different sunshine states</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch18-ex09">
<p><ol marker="a.">
  <li><p>The two variables are independent</p></li>
  <li><p>The null hypothesis is that the variables are independent; the alternative hypothesis is that the variables are associated. The p-value is around 0. There is convincing evidence to claim that site and sunlight preference are associated</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch18-ex11">
<p><ol marker="a.">
  <li><p>False. The Chi-square distribution has one parameter called degrees of freedom</p></li>
  <li><p>True</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch18-ex13">
<p>The hypotheses are <m>H_0:</m> Sleep levels and profession are independent. <m>H_A:</m> Sleep levels and profession are associated. The observations are independent and the sample sizes are large enough to conduct a Chi-square test of independence. The Chi-square statistic is 1 with 2 degrees of freedom. The p-value is 0.6. Since the p-value is high (default to alpha = 0.05), we fail to reject <m>H_0</m>. The data do not provide convincing evidence of an association between sleep levels and profession.

    </p>
    </solution>

    <solution xml:id="sol-ch18-ex15">
<p><ol marker="a.">
  <li><p><m>H_0</m>: The age of Los Angeles residents is independent of shipping carrier preference variable. <m>H_A</m>: The age of Los Angeles residents is associated with the shipping carrier preference variable</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-19">
    <title>Chapter 19</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp19-exercises" />.
    </p>

    <solution xml:id="sol-ch19-ex01">
<p><ol marker="a.">
  <li><p>Average sleep of 25 in sample vs. all New Yorkers</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex03">
<p><ol marker="a.">
  <li><p>Use the sample mean to estimate the population mean: 171.1. Likewise, use the sample median to estimate the population median: 170.3</p></li>
  <li><p><m>Z_{180} = 0.95</m> and <m>Z_{155} = -1.71.</m> Neither of these observations is more than two standard deviations away from the mean, so neither would be considered unusual</p></li>
  <li><p>We use the standard error of the mean to measure the variability in means of random samples of same size taken from a population. The variability in the means of random samples is quantified by the standard error. Based on this sample, <m>SE_{\bar{x}} = \frac{9.4}{\sqrt{507}} = 0.417.</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex05">
<p><ol marker="a.">
  <li><p>The kindergartners will have a smaller standard deviation of heights. We would expect their heights to be more similar to each other compared to a group of adults' heights</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex07">
<p><ol marker="a.">
  <li><p><m>df=6-1=5</m>, <m>t_{5}^{\star} = 2.02</m></p></li>
  <li><p><m>df=28</m>, <m>t_{28}^{\star} = 2.05</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex09">
<p><ol marker="a.">
  <li><p>0.085, do not reject <m>H_0</m></p></li>
  <li><p>0.438, do not reject <m>H_0</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex11">
<p><ol marker="a.">
  <li><p>Roughly 0.1 weeks</p></li>
  <li><p>Roughly (38.49 weeks, 38.91 weeks)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex13">
<p><ol marker="a.">
  <li><p>False</p></li>
  <li><p>True</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex15">
<p>The mean is the midpoint: <m>\bar{x} = 20</m>. Identify the margin of error: <m>ME = 1.015</m>, then use <m>t^{\star}_{35} = 2.03</m> and <m>SE = s/ \sqrt{n}</m> in the formula for margin of error to identify <m>s = 3</m>.
</p>
    </solution>

    <solution xml:id="sol-ch19-ex17">
<p><ol marker="a.">
  <li><p><m>H_0</m>: <m>\mu = 8</m> (New Yorkers sleep 8 hrs per night on average.) 
<m>H_A</m>: <m>\mu \neq 8</m> (New Yorkers sleep less or more than 8 hrs per night on average.)</p></li>
  <li><p>p-value <m>= 0.093</m>. If in fact the true population mean of the amount New Yorkers sleep per night was 8 hours, the probability of getting a random sample of 25 New Yorkers where the average amount of sleep is 7.73 hours per night or less (or 8.27 hours or more) is 0.093</p></li>
  <li><p>Yes, since we did not rejected <m>H_0</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch19-ex19">
<p>With a larger critical value, the confidence interval ends up being wider. This makes intuitive sense as when we have a small sample size and the population standard deviation is unknown, we should have a wider interval than if we knew the population standard deviation, or if we had a large enough sample size.
</p>
    </solution>

    <solution xml:id="sol-ch19-ex21">
<p><ol marker="a.">
  <li><p>We will conduct a 1-sample <m>t</m>-test. <m>H_0</m>: <m>\mu = 5</m>. <m>H_A</m>: <m>\mu \neq 5</m>. We'll use <m>\alpha = 0.05</m>. This is a random sample, so the observations are independent. To proceed, we assume the distribution of years of piano lessons is approximately normal. <m>SE = 2.2 / \sqrt{20} = 0.4919</m>. The test statistic is <m>T = (4.6 - 5) / SE = -0.81</m>. <m>df = 20 - 1 = 19</m>. The one-tail area is about 0.21, so the p-value is about 0.42, which is bigger than <m>\alpha = 0.05</m> and we do not reject <m>H_0</m>. That is, we do not have sufficiently strong evidence to reject the notion that the average is 5 years</p></li>
  <li><p>They agree, since we did not reject the null hypothesis and the null value of 5 was in the <m>t</m>-interval</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-20">
    <title>Chapter 20</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp20-exercises" />.
    </p>

    <solution xml:id="sol-ch20-ex01">
<p>The hypotheses should use population means (<m>\mu</m>) not sample means (<m>\bar{x}</m>), the null hypothesis should set the two population means equal to each other, the alternative hypothesis should be two-tailed and use a not equal to sign.
</p>
    </solution>

    <solution xml:id="sol-ch20-ex03">
<p><m>H_0: \mu_{0.99} = \mu_{1}</m> and <m>H_A: \mu_{0.99} \ne \mu_{1}.</m> p-value <m>&lt;</m> 0.05, reject <m>H_0.</m> The data provide convincing evidence that the difference in population averages of price per carat of 0.99 carats and 1 carat diamonds are different.
</p>
    </solution>

    <solution xml:id="sol-ch20-ex05">
<p><ol marker="a.">
  <li><p>We are 95% confident that the population average price per carat of 0.99 carat diamonds is \<m>2 to \</m>23 lower than the population average price per carat of 1 carat diamonds</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch20-ex07">
<p>The difference is not zero (statistically discernible), but there is no evidence that the difference is large (practically important), because the interval provides values as low as 1 lb.
</p>
    </solution>

    <solution xml:id="sol-ch20-ex09">
<p><m>H_0: \mu_{0.99} = \mu_{1}</m> and <m>H_A: \mu_{0.99} \ne \mu_{1}</m>. Independence: Both samples are random and represent less than 10% of their respective populations. Also, we have no reason to think that the 0.99 carats are not independent of the 1 carat diamonds since they are both sampled randomly. Normality: The distributions are not extremely skewed, hence we can assume that the distribution of the average differences will be nearly normal as well. <m>T_{22} = -2.7</m>, p-value = 0.0131. Since p-value less than 0.05, reject <m>H_0</m>. The data provide convincing evidence that the difference in population averages of price per carat of 0.99 carats and 1 carat diamonds are different.
</p>
    </solution>

    <solution xml:id="sol-ch20-ex11">
<p>We are 95% confident that the population average price per carat of 0.99 carat diamonds is \<m>2.96 to \</m>22.42 lower than the population average price per carat of 1 carat diamonds.
</p>
    </solution>

    <solution xml:id="sol-ch20-ex13">
<p><ol marker="a.">
  <li><p><m>\mu_{\bar{x}_1} = 15</m>, <m>\sigma_{\bar{x}_1} = 20 / \sqrt{50} = 2.8284.</m></p></li>
  <li><p><m>\mu_{\bar{x}_2 - \bar{x}_1} = 20 - 15 = 5</m>, <m>\sigma_{\bar{x}_2 - \bar{x}_1} = \sqrt{\left(20 / \sqrt{50}\right)^2 + \left(10 / \sqrt{30}\right)^2} = 3.3665.</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch20-ex15">
<p><ol marker="a.">
  <li><p>Chicken fed linseed weighed an average of 218.75 grams while those fed horsebean weighed an average of 160.20 grams. Both distributions are relatively symmetric with no apparent outliers. There is more variability in the weights of chicken fed linseed</p></li>
  <li><p>Type I error, since we rejected <m>H_0</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch20-ex17">
<p><m>H_0: \mu_C = \mu_S</m>. <m>H_A: \mu_C \ne \mu_S</m>. <m>T = 3.27</m>, <m>df=11</m> <m>\to</m> p-value <m>= 0.007</m>. Since p-value <m>&lt; 0.05</m>, reject <m>H_0</m>. The data provide strong evidence that the average weight of chickens that were fed casein is different than the average weight of chickens that were fed soybean (with weights from casein being higher). Since this is a randomized experiment, the observed difference can be attributed to the diet.
</p>
    </solution>

    <solution xml:id="sol-ch20-ex19">
<p><m>H_0: \mu_{T} = \mu_{C}</m>. <m>H_A: \mu_{T} \ne \mu_{C}</m>. <m>T=2.24</m>, <m>df=21</m> <m>\to</m> p-value <m>= 0.036</m>. Since p-value <m>&lt;</m> 0.05, reject <m>H_0</m>. The data provide strong evidence that the average food consumption by the patients in the treatment and control groups are different. Furthermore, the data indicate patients in the distracted eating (treatment) group consume more food than patients in the control group.
</p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-21">
    <title>Chapter 21</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp21-exercises" />.
    </p>

    <solution xml:id="sol-ch21-ex01">
<p>Paired, data are recorded in the same cities at two different time points. The temperature in a city at one point is not independent of the temperature in the same city at another time point
</p>
    </solution>

    <solution xml:id="sol-ch21-ex03">
<p><ol marker="a.">
  <li><p>Since it's the same students at the beginning and the end of the semester, there is a pairing between the datasets, for a given student their beginning and end of semester grades are dependent</p></li>
  <li><p>Since it's the same subjects at the beginning and the end of the study, there is a pairing between the datasets, for a subject student their beginning and end of semester artery thickness are dependent</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch21-ex05">
<p>False. While it is true that paired analysis requires equal sample sizes, only having the equal sample sizes isn't, on its own, sufficient for doing a paired test. Paired tests require that there be a special correspondence between each pair of observations in the two groups.
</p>
    </solution>

    <solution xml:id="sol-ch21-ex07">
<p><ol marker="a.">
  <li><p>Let <m>diff = 2022 - 1950</m>. Then the hypotheses are <m>H_0: \mu_{diff} = 0</m> and <m>H_A: \mu_{diff} \ne 0</m></p></li>
  <li><p>Since the p-value <m>&lt;</m> 0.05, reject <m>H_0</m>. There is evidence of a difference between the average 90<m>^{th}</m> percentile high temperature in 2022 and the average 90<m>^{th}</m> percentile high temperature in 1950</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch21-ex09">
<p><ol marker="a.">
  <li><p>Roughly (1.5<m>^\circ</m>F, 3.5<m>^\circ</m>F)</p></li>
  <li><p>We are 90% confident that the true average of the difference in 90<m>^{th}</m> percentile high temperature in 2022 vs 1950 is somewhere between 1.5<m>^\circ</m>F and 3.5<m>^\circ</m>F. We are 90% confident that the true average of the difference in 90<m>^{th}</m> percentile high temperature in 2022 vs 1950 is somewhere between 1.5<m>^\circ</m>F and 3.56<m>^\circ</m>F</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch21-ex11">
<p><ol marker="a.">
  <li><p>For each observation in the 1950 dataset, there is exactly one specially corresponding observation in the 2022 dataset for the same geographic location. The data are paired</p></li>
  <li><p>Locations were not randomly sampled across the geographic region, so we need to be careful concluding independence. However, the question above describes the data as representative of the land area of the lower 48 states, so independence is reasonable. The sample size is 26 which is close to 30, so we're just looking for particularly extreme outliers: none are present (the observation off to the right in the histogram would be considered a outlier, but not a particularly extreme one). Therefore, the conditions are reasonably satisfied</p></li>
  <li><p>Since the p-value is less than 0.05, we reject <m>H_0</m>. The data provide strong evidence that NOAA stations observed a hotter 90<m>^{th}</m> percentile high temperature in 2022 than in 1950</p></li>
  <li><p>No, since we rejected <m>H_0</m>, which had a null value of 0</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch21-ex13">
<p><ol marker="a.">
  <li><p><m>SE = 0.579</m> and <m>t^{\star}_{25} = 1.71</m>. <m>2.53 \pm 1.71 \times 0.579 \to (1.54</m>^\circ<m>F, 3.52</m>^\circ<m>F)</m></p></li>
  <li><p>Yes, since the interval lies entirely above 0</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch21-ex15">
<p><ol marker="a.">
  <li><p>Each student study under each condition, use the difference in individual student scores</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch21-ex17">
<p><ol marker="a.">
  <li><p>(-6.49, -0.17)</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-22">
    <title>Chapter 22</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp22-exercises" />.
    </p>

    <solution xml:id="sol-ch22-ex01">
<p>Alternative.
</p>
    </solution>

    <solution xml:id="sol-ch22-ex03">
<p><ol marker="a.">
  <li><p>Means across original data are more variable</p></li>
  <li><p>F statistic is bigger for the original data</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch22-ex05">
<p><m>H_0</m>: <m>\mu_1 = \mu_2 = \cdots = \mu_6</m>. <m>H_A</m>: The average weight varies across some (or all) groups. Independence: Chicks are randomly assigned to feed types (presumably kept separate from one another), therefore independence of observations is reasonable. Approx. normal: the distributions of weights within each feed type appear to be fairly symmetric. Constant variance: Based on the side-by-side box plots, the constant variance assumption appears to be reasonable. There are differences in the actual computed standard deviations, but these might be due to chance as these are quite small samples. <m>F_{5,65} = 15.36</m> and the p-value is approximately 0. With such a small p-value, we reject <m>H_0</m>. The data provide convincing evidence that the average weight of chicks varies across some (or all) feed supplement groups.
</p>
    </solution>

    <solution xml:id="sol-ch22-ex07">
<p><ol marker="a.">
  <li><p><m>H_0</m>: The population mean of MET for each group is equal to the others. <m>H_A</m>: At least one pair of means is different</p></li>
  <li><p>Since p-value is very small, reject <m>H_0</m>. The data provide convincing evidence that the average MET differs between at least one pair of groups</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch22-ex09">
<p><ol marker="a.">
  <li><p><m>H_0</m>: Average GPA is the same for all majors. <m>H_A</m>: At least one pair of means are different</p></li>
  <li><p>The total degrees of freedom is <m>195 + 2 = 197</m>, so the sample size is <m>197+1=198</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch22-ex11">
<p><ol marker="a.">
  <li><p>False. As the number of groups increases, so does the number of comparisons and hence the modified discernibility level decreases</p></li>
  <li><p>True</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch22-ex13">
<p><ol marker="a.">
  <li><p>Left is Dataset B</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-23">
    <title>Chapter 23</title>
    <p>
      Application chapter, no exercises.
    </p>
  </section>
  <section xml:id="sec-exercise-solutions-24">
    <title>Chapter 24</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp24-exercises" />.
    </p>

    <solution xml:id="sol-ch24-ex01">
<p><ol marker="a.">
  <li><p><m>H_0: \beta_1 = 0</m>, <m>H_A: \beta_1 \ne 0</m></p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex03">
<p><ol marker="a.">
  <li><p>The relationship is positive, moderate-to-strong, and linear. There are a few outliers but no points that appear to be influential</p></li>
  <li><p><m>H_0</m>: The true slope coefficient of height is zero (<m>\beta_1 = 0</m>). <m>H_A</m>: The true slope coefficient of height is different than zero (<m>\beta_1 \neq 0</m>). The p-value for the two-sided alternative hypothesis (<m>\beta_1 \ne 0</m>) is incredibly small, so we reject <m>H_0</m>. The data provide convincing evidence that height and weight are positively correlated. The true slope parameter is indeed greater than 0</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex05">
<p><ol marker="a.">
  <li><p>Roughly 0.53 to 0.67</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex07">
<p><ol marker="a.">
  <li><p>Approximately 0.025</p></li>
  <li><p>For individuals with one cm larger shoulder girth, their average height is predicted to be between 0.546 and 0.662 cm taller, with 98% confidence</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex09">
<p><ol marker="a.">
  <li><p><m>r = \sqrt{0.518} \approx +0.72</m>. We know the correlation is positive due to the positive association between the variables seen in the scatterplot (above in previous exercise)</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex11">
<p><ol marker="a.">
  <li><p><m>H_0: \beta_1 = 0</m>, <m>H_A: \beta_1 \ne 0</m></p></li>
  <li><p>The p-value is also extremely small</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex13">
<p><ol marker="a.">
  <li><p>Rough 90% confidence interval is 1.9 to 3.1</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex15">
<p><ol marker="a.">
  <li><p><m>r = \sqrt{0.706} \approx +0.84</m>. We know the correlation is positive due to the positive association shown in the scatterplot</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex17">
<p><ol marker="a.">
  <li><p>With only sixteen observations in the analysis there are not enough data points to establish any patterns in the residual plot. That said, the sixteen observations do not show any large deviations of LNE conditions.  We do not know if the volunteers were friends, for example, which would violate the independence condition</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch24-ex19">
<p><ol marker="a.">
  <li><p>The **L**inearity and **N**ormality conditions seem to be met.  If anything, the **E**qual variance condition is violated due to the a fan shaped pattern in the plot, which indicates non-constant variability in the residuals (little variability when <m>x</m> is small, more variability when <m>x</m> is large). We do not know if the cats were randomly sampled (i.e., are independent from one another), but we have no reason to believe that they are not independent</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-25">
    <title>Chapter 25</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp25-exercises" />.
    </p>

    <solution xml:id="sol-ch25-ex01">
<p><ol marker="a.">
  <li><p>(-0.044, 0.346). We are 95% confident that student who go out more than two nights a week on average have GPAs 0.044 points lower to 0.346 points higher than those who do not go out more than two nights a week, when controlling for the other variables in the model</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch25-ex03">
<p><ol marker="a.">
  <li><p>`volume` and `diam`; `volume` and `height`; `diam` and `height`</p></li>
  <li><p>When both `diam`eter and `height` are used in the multiple linear regression model, both continue to be discernible predictors of `volume`</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch25-ex05">
<p><ol marker="a.">
  <li><p>**L**inearity: Horror movies seem to show a much different pattern than the other genres. While the residuals plots show a random scatter over years and in order of data collection, there is a clear pattern in residuals for various genres, which signals that this regression model is not appropriate for these data. **I**ndependent observations: The variability of the residuals is higher for data that comes later in the dataset. We don't know if the data are sorted by year, but if so, there may be a temporal pattern in the data that voilates the independence condition. **N**ormality: The residuals are right skewed (skewed to the high end). Constant or **E**qual variability: The residuals vs. predicted values plot reveals some outliers. This plot for only babies with predicted birth weights between 6 and 8.5 pounds looks a lot better, suggesting that for bulk of the data the constant variance condition is met</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch25-ex07">
<p><ol marker="a.">
  <li><p>Linearity: With so many observations in the dataset, we look for particularly extreme outliers in the histogram of residuals and do not see any. We also don't see a non-linear pattern emerging in the residuals vs. predicted plot. Independent observations: The sample is random and there does not seem to be a trend in the residuals vs. order of data collection plot. Normality: The histogram of residuals appears to be unimodal and symmetic, centered at 0. Constant or equal variability: The residuals vs. predicted values plot reveals some outliers. This plot for only babies with predicted birth weights between 6 and 8.5 pounds looks a lot better, suggesting that for bulk of the data the constant variance condition is met. All concerns raised here are relatively mild. There are some outliers, but there is so much data that the influence of such observations will be minor</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch25-ex09">
<p><ol marker="a.">
  <li><p>Roughly <m>\widehat{\texttt{weight}} = 11</m> pounds and <m>\texttt{weight}_i = 7</m> pounds</p></li>
  <li><p>The plot on the top estimates 8 parameters; the plot on the bottom estimates 3 parameters</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch25-ex11">
<p><ol marker="a.">
  <li><p>The plots are difficult to differentiate</p></li>
  <li><p>The model with more predictors seems to be over-fitting the data used to model build at the expense of not fitting (as well) the cross-validation hold out set for prediction</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-26">
    <title>Chapter 26</title>
    <p>
      Solutions for odd-numbered exercises in <xref ref="sec-chp26-exercises" />.
    </p>

    <solution xml:id="sol-ch26-ex01">
<p>No, logistic regression is not appropriate because the response (or outcome) variable is not binary. Linear regression is likely to be more appropriate.
</p>
    </solution>

    <solution xml:id="sol-ch26-ex03">
<p><m>H_0: \beta_1 = 0</m>, the slope of the model predicting kids' marijuana use in college from their parents' marijuana use in college is 0. <m>H_A: \beta_1 \neq 0</m>, the slope of the model predicting kids' marijuana use in college from their parents' marijuana use in college is different than 0. The test statistic is <m>Z = 4.09</m> and the associated p-value is less than 0.0001. With a small p-value we reject <m>H_0</m>. The data provide convincing evidence that the slope of the model predicting kids' marijuana use in college from their parents' marijuana use in college is different than 0, i.e., that parents' marijuana use in college is a discernible predictor of kids' marijuana use in college.
</p>
    </solution>

    <solution xml:id="sol-ch26-ex05">
<p><ol marker="a.">
  <li><p>26 observations are in Fold2. 8 correctly and 2 incorrectly predicted to be from Victoria</p></li>
  <li><p>2 coefficients for tail length; 3 coefficients for total length and sex</p></li>
</ol></p>
    </solution>

    <solution xml:id="sol-ch26-ex07">
<p><ol marker="a.">
  <li><p>76, 73.1%</p></li>
  <li><p>The tail length model should be chosen for classification purposes</p></li>
</ol></p>
    </solution>

  </section>
  <section xml:id="sec-exercise-solutions-27">
    <title>Chapter 27</title>
    <p>
      Application chapter, no exercises.
    </p>
  </section>
</appendix>
