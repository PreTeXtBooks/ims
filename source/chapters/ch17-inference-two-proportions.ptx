<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch17-inference-two-proportions">
  <title>Inference for comparing two proportions</title>

  <introduction>
    <p>We now extend the methods from <xref ref="ch16-inference-single-proportion" text="title" /> to apply confidence intervals and hypothesis tests to differences in population proportions that come from two groups, Group 1 and Group 2: <m>p_1 - p_2.</m></p>
    <p>In our investigations, we'll identify a reasonable point estimate of <m>p_1 - p_2</m> based on the sample, and you may have already guessed its form: <m>\hat{p}_1 - \hat{p}_2.</m> <idx><h>point estimate</h><h>difference of proportions</h></idx><idx><h>difference of proportions</h><h>point estimate</h></idx> Then we'll look at the inferential analysis in three different ways: using a randomization test, applying bootstrapping for interval estimates, and, if we verify that the point estimate can be modeled using a normal distribution, we compute the estimate's standard error, and we apply the mathematical framework.</p>
  </introduction>
  
  <section xml:id="sec-two-prop-errors">
    <title>Randomization test for the difference in proportions</title>
    
    <subsection>
      <title>Observed data</title>
      
      <p>Let's take another look at the cardiopulmonary resuscitation (CPR) study we introduced in <xref ref="sec-two-sided-hypotheses" />.</p>
      
      <p>The experiment consisted of two treatments on patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.</p>
      
      <p>Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group).</p>
      
      <p>The outcome variable of interest was whether the patient survived for at least 24 hours.</p>
      
      <p><xref ref="biblio-Bottiger-2001" /></p>
      
      <note>
        <title>Data</title>
        <p>The <url href="http://openintrostat.github.io/openintro/reference/cpr.html"><c>cpr</c></url> data can be found in the <url href="http://openintrostat.github.io/openintro"><alert>openintro</alert></url> R package.</p>
      </note>
      
      <p>The results are summarized in <xref ref="tbl-cpr-summary-again" /> (which is a replica of <xref ref="tbl-cpr-summary" />).</p>
      
      <p>11 out of the 50 patients in the control group and 14 out of the 40 patients in the treatment group survived.</p>
      
      <table xml:id="tbl-cpr-summary-again">
        <title>Results for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not. tbl-pos: H</title>
        <tabular>
          <row><cell>Table content</cell></row>
        </tabular>
      </table>
      
      <exercise>
        <statement>
          <p>Is this an observational study or an experiment?</p>
          <p>What implications does the study type have on what can be inferred from the results?</p>
        </statement>
        <solution>
          <p>The study is an experiment, as patients were randomly assigned an experiment group.</p>
          <p>Since this is an experiment, the results can be used to evaluate a causal relationship between blood thinner use after CPR and whether patients survived.</p>
        </solution>
      </exercise>
      
      <p>In this study, a larger proportion of patients who received blood thinner after CPR,<m>\hat{p}_T = \frac{14}{40} = 0.35,</m> survived compared to those who did not receive blood thinner, <m>\hat{p}_C = \frac{11}{50} = 0.22.</m> However, based on these observed proportions alone, we cannot determine whether the difference (<m>\hat{p}_T - \hat{p}_C = 0.35 - 0.22 = 0.13</m>) provides <em>convincing evidence</em> that blood thinner usage after CPR is effective.</p>
      
      <p>As we saw in <xref ref="ch11-hypothesis-testing-randomization" text="title" />, we can re-randomize the responses (<c>survived</c> or <c>died</c>) to the treatment conditions assuming the null hypothesis is true and compute possible differences in proportions.</p>
      
      <p>The process by which we randomize observations to two groups is summarized and visualized in <xref ref="fig-fullrand" />).</p>
      
    </subsection>
    
    <subsection>
      <title>Variability of the statistic</title>
      
      <p><xref ref="fig-cpr-rand-dot-plot" /> shows a stacked plot of the differences found from 100 randomization simulations (i.e., repeated iterations as described in <xref ref="fig-fullrand" />, where each dot represents a simulated difference between the infection rates (control rate minus treatment rate).</p>
      
      <figure xml:id="fig-cpr-rand-dot-plot">
        <caption>A stacked dot plot of differences from 100 simulations produced under the independence model <m>H_0,</m> where in these simulations survival is unaffected by the treatment. Twelve of the 100 simulations had a difference of at least 13%, the difference observed in the study. fig-alt: | A stacked dot plot of differences from 100 simulations produced under the independence model <m>H_0,</m> where in these simulations survival is unaffected by the treatment. Twelve of the 100 simulations had a difference of at least 13%, the difference observed in the study. fig-asp: 0.5</caption>
        <image source="images/fig-cpr-rand-dot-plot-1.png" width="70%" />
      </figure>
      
    </subsection>
    
    <subsection>
      <title>Observed statistic vs null statistics</title>
      
      <p>Note that the distribution of these simulated differences is centered around 0.</p>
      
      <p>We simulated the differences assuming that the independence model was true, that blood thinners after CPR have no effect on survival.</p>
      
      <p>Under the null hypothesis, we expect the difference to be near zero with some random fluctuation, where <em>near</em> is pretty generous in this case since the sample sizes are so small in this study.</p>
      
      <example>
        <statement>
          <p>How often would you observe a difference of at least 13% (0.13) according to <xref ref="fig-cpr-rand-dot-plot" />?</p>
          <p>Is this a rare event?</p>
        </statement>
        <solution>
          <p>It appears that a difference of at least 13% due to chance alone, if the null hypothesis was true would happen about 12% of the time according to <xref ref="fig-cpr-rand-dot-plot" />.</p>
          <p>This is not a very rare event.</p>
        </solution>
      </example>
      
      <p>The difference of 13% not being a rare event suggests two possible interpretations of the results of the study:</p>
      
      <ul>
        <li><p>-   <m>H_0</m> Independence model. Blood thinners after CPR have no effect on survival, and we just happened to observe a difference that would only occur on a rare occasion.</p></li>
        <li><p>-   <m>H_A</m> Alternative model. Blood thinners after CPR increase chance of survival, and the difference we observed was actually due to the blood thinners after CPR being effective at increasing the chance of survival, which explains the difference of 13%.</p></li>
      </ul>
      
      <p>Since we determined that the outcome is not that rare (12% chance of observing a difference of 13% or more under the assumption that blood thinners after CPR have no effect on survival), we fail to reject <m>H_0</m>, and conclude that the study results do not provide strong evidence against the independence model.</p>
      
      <p>This does not mean that we have proved that blood thinners are not effective, it just means that this study does not provide convincing evidence that they are effective in this setting.</p>
      
      <p>Statistical inference, is built on evaluating how likely such differences are to occur due to chance if in fact the null hypothesis is true.</p>
      
      <p>In statistical inference, data scientists evaluate which model is most reasonable given the data.</p>
      
      <p>Errors do occur, just like rare events, and we might choose the wrong model.</p>
      
      <p>While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often these errors occur.</p>
      
    </subsection>
    
  </section>
  
  <section xml:id="sec-two-prop-boot-ci">
    <title>Bootstrap confidence interval for the difference in proportions</title>
    
    <p></p>
    
    <p>In <xref ref="sec-two-prop-errors" />, we worked with the randomization distribution to understand the distribution of <m>\hat{p}_1 - \hat{p}_2</m> when the null hypothesis <m>H_0: p_1 - p_2 = 0</m> is true.</p>
    
    <p>Now, through bootstrapping, we study the variability of <m>\hat{p}_1 - \hat{p}_2</m> without assuming the null hypothesis is true.</p>
    
    <subsection>
      <title>Observed data</title>
      
      <p>Reconsider the CPR data from <xref ref="sec-two-prop-errors" /> which is provided in  <xref ref="tbl-cpr-summary" />.</p>
      
      <p>Again, we use the difference in sample proportions as the observed statistic of interest.</p>
      
      <p>Here, the value of the statistic is: <m>\hat{p}_T - \hat{p}_C = 0.35 - 0.22 = 0.13.</m></p>
      
    </subsection>
    
    <subsection>
      <title>Variability of the difference in sample proportions</title>
      
      <p>The bootstrap method applied to two samples is an extension of the method described in <xref ref="ch12-confidence-intervals-bootstrapping" text="title" />.</p>
      
      <p>Now, we have two samples, so each sample estimates the population from which they came.</p>
      
      <p>In the CPR setting, the <c>treatment</c> sample estimates the population of all individuals who have gotten (or will get) the treatment; the <c>control</c> sample estimates the population of all individuals who do not get the treatment and are controls.</p>
      
      <p><xref ref="fig-boot2proppops" /> extends <xref ref="fig-boot1" /> to show the bootstrapping process from two samples simultaneously.</p>
      
      <figure xml:id="fig-boot2proppops">
        <caption>out-width: 100% fig-alt: | Sample 1 is taken from Population 1 (3 colored marbles out of 7); Sample 2 is taken from Population 2 (5 colored marbles out of 9). Each of the two samples is used to create separate infinitely large proxy populations. Proxy population 1 has 3/7 colored marbles; proxy population 2 has 4/9 colored marbles.</caption>
        <image source="images/fig-boot2proppops-1.png" width="70%" />
      </figure>
      
      <p>As before, once the population is estimated, we can randomly resample observations to create bootstrap samples, as seen in <xref ref="fig-boot2propresamps" />.</p>
      
      <figure xml:id="fig-boot2propresamps">
        <caption>out-width: 100% fig-alt: | Sample 1 is taken from Population 1 (3 colored marbles out of 7); Sample 2 is taken from Population 2 (5 colored marbles out of 9). Each of the two samples is used to create separate infinitely large proxy populations. Proxy population 1 has 3/7 colored marbles; proxy population 2 has 4/9 colored marbles. Resamples are taken from each of the proxy populations. The three resamples from proxy population 1 have 2/7, 4/7 and 5/7 colored marbles, respectively. The three resamples from proxy population 2 have 5/9, 4/9, and 7/9 colored smarbles, respectively.</caption>
        <image source="images/fig-boot2propresamps-1.png" width="70%" />
      </figure>
      
      <p>The variability of the statistic (the difference in sample proportions) can be calculated by taking one bootstrap resample from Sample 1 and one bootstrap resample from Sample 2 and calculating the difference in the bootstrap proportions.</p>
      
      <figure xml:id="fig-boot2samp2">
        <caption>For example, the first bootstrap resamples from Sample 1 and Sample 2 provide resample proportions of 2/7 and 5/9, respectively. out-width: 60% fig-alt: | The first resamples from each of the two proxy populations are compared. Resample 1 from proxy population 1 has 2/7 colored marbles; resample 1 from proxy population 2 has 5/9 colored marbles. The difference in bootstrap proportions is taken as 2/7 minus 5/9.</caption>
        <image source="images/fig-boot2samp2-1.png" width="70%" />
      </figure>
      
      <p>As always, the variability of the difference in proportions can only be estimated by repeated simulations, in this case, repeated bootstrap resamples.</p>
      
      <p><xref ref="fig-boot2samp2" /> shows multiple bootstrap differences calculated for each of the repeated bootstrap samples.</p>
      
      <figure xml:id="fig-boot2samp3">
        <caption>For each pair of bootstrap samples, we calculate the difference in sample proportions. out-width: 100% fig-alt: | Shown are the two infinitely large proxy populations (created from sample 1 and sample 2). From each proxy population, three resamples are shown. For each pair of resamples, the difference in bootstrap proportions is taken. The first pair of resamples gives a difference in bootstrapped proportions of 2/7 minus 5/9; the second pair of resamples gives a difference in bootstrapped proportions of 4/7 minus 4/9; the last pair of resamples gives a difference in bootstrapped proportions of 5/7 minus 7/9. fig-asp: 0.6</caption>
        <image source="images/fig-boot2samp3-1.png" width="70%" />
      </figure>
      
      <p>Repeated bootstrap simulations lead to a bootstrap sampling distribution of the statistic of interest, here the difference in sample proportions.</p>
      
      <p><xref ref="fig-boot2samp1" /> visualizes the process and <xref ref="fig-bootCPR1000" /> shows 1,000 bootstrap differences in proportions for the CPR data.</p>
      
      <p>Note that the CPR data includes 40 and 50 people in the respective groups, and the illustrated example includes 7 and 9 people in the two groups.</p>
      
      <p>Accordingly, the variability in the distribution of sample proportions is higher for the illustrated example.</p>
      
      <p>As you will see in the mathematical models discussed in <xref ref="sec-math-2prop" />, large sample sizes lead to smaller standard errors for a difference in proportions.</p>
      
      <figure xml:id="fig-boot2samp1">
        <caption>The differences in each bootstrapped pair of proportions are combined to create the sampling distribution of the differences in proportions. out-width: 100% fig-alt: | Shown are the two infinitely large proxy populations (created from sample 1 and sample 2). From each proxy population, three resamples are shown. For each pair of resamples, the difference in bootstrap proportions is taken. A dotplot displays many differences in bootstrap proportions. The differences range from roughly -0.6 to +0.3. fig-asp: 0.6</caption>
        <image source="images/fig-boot2samp1-1.png" width="70%" />
      </figure>
      
      <figure xml:id="fig-bootCPR1000">
        <caption>A histogram of differences in proportions from 1,000 bootstrap simulations of the CPR data. Note that because the CPR data has a larger sample size than the illustrated example, the variability of the difference in proportions is much smaller with the CPR histogram. fig-alt: | A histogram of differences in proportions from 1000 bootstrap simulations of the CPR data. fig-asp: 0.5</caption>
        <image source="images/fig-bootCPR1000-1.png" width="70%" />
      </figure>
      
    </subsection>
    
    <subsection>
      <title>Bootstrap percentile vs. SE confidence intervals</title>
      
      <p><xref ref="fig-bootCPR1000" /> provides an estimate for the variability of the difference in survival proportions from sample to sample.</p>
      
      <p>The values in the histogram can be used in two different ways to create a confidence interval for the parameter of interest: <m>p_1 - p_2</m>.</p>
      
      <p>As in <xref ref="ch12-confidence-intervals-bootstrapping" text="title" />, the bootstrap confidence interval can be calculated directly from the bootstrapped differences in <xref ref="fig-bootCPR1000" />.</p>
      
      <p>The interval created from the percentiles of the distribution is called the <alert>percentile interval</alert><idx><h>bootstrap</h><h>percentile interval</h></idx><idx>percentile interval</idx>.</p>
      
      <p>Note that here we calculate the 90% confidence interval by finding the <m>5^{th}</m> and <m>95^{th}</m> percentile values from the bootstrapped differences.</p>
      
      <p>The bootstrap 5 percentile proportion is <c>r cpr_boot_90_lower_perc</c> and the 95 percentile is <c>r cpr_boot_90_upper_perc</c>.</p>
      
      <p>The result is: we are 90% confident that, in the population, the true difference in probability of survival for individuals receiving blood thinners after CPR is between <c>r cpr_boot_90_lower_perc</c> lower and <c>r cpr_boot_90_upper_perc</c> higher than those who did not receive blood thinners.</p>
      
      <p>The interval shows that we do not have much definitive evidence of the effect of blood thinners, one way or another.</p>
      
      <figure xml:id="fig-bootCPR1000CI">
        <caption>The CPR data is bootstrapped 1,000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is <m>\hat{p}_{T}  = 14/40</m> and the probability of survival in the control group is <m>\hat{p}_{C} = 11/50.</m> fig-alt: | A histogram of differences in proportions from 1000 bootstrap simulations of the CPR data. The 5th and 95th percentiles are shown as vertical lines.</caption>
        <image source="images/fig-bootCPR1000CI-1.png" width="70%" />
      </figure>
      
      <p>Alternatively, we can use the variability in the bootstrapped differences to calculate a standard error of the difference.</p>
      
      <p>The resulting interval is called the <alert>SE interval</alert><idx>SE interval</idx><idx><h>bootstrap</h><h>SE interval</h></idx>.</p>
      
      <p><xref ref="sec-math-2prop" /> details the mathematical model for the standard error of the difference in sample proportions, but the bootstrap distribution typically does an excellent job of estimating the variability of the sampling distribution of the sample statistic.</p>
      
      <me>
        SE(\hat{p}_T - \hat{p}_C) \approx SE(\hat{p}_{T, boot} - \hat{p}_{C, boot}) = <c>r cpr_boot_se</c>
      </me>
      
      <p>The variability of the difference in proportions was calculated in R using the <c>sd()</c> function, but any statistical software will calculate the standard deviation of the differences, here, the exact quantity we hope to approximate.</p>
      
      <p>Note that we do not know the true distribution of <m>\hat{p}_T - \hat{p}_C,</m> so we will use a rough approximation to find a confidence interval for <m>p_T - p_C.</m> As seen in the bootstrap histograms, the shape of the distribution is roughly symmetric and bell-shaped.</p>
      
      <p>So for a rough approximation, we will apply the 67-95-99.7 rule which tells us that 95% of observed differences should be roughly no farther than 2 SE from the true parameter (difference in proportions).</p>
      
      <p>A 95% confidence interval for <m>p_T - p_C</m> is given by:</p>
      
      <me>
        \hat{p}_T - \hat{p}_C \pm 2 \cdot SE \rightarrow \ \ \ 14/40 - 11/50 \pm 2 \cdot <c>r cpr_boot_se</c> \ \ \  \rightarrow \ \ \  (<c>r cpr_boot_90_lower_se</c>, <c>r cpr_boot_90_upper_se</c>)
      </me>
      
      <p>We are 95% confident that the true value of <m>p_T - p_C</m> is between <c>r cpr_boot_90_lower_se</c> and <c>r cpr_boot_90_upper_se</c>.</p>
      
      <p>Again, the wide confidence interval that contains zero indicates that the study provides very little evidence about the effectiveness of blood thinners.</p>
      
      <p>For other percentages, e.g., a 90% bootstrap SE confidence interval, we will use quantiles given by the standard normal distribution, as seen in <xref ref="sec-normalDist" /> and <xref ref="fig-er6895997" />.</p>
      
    </subsection>
    
    <subsection>
      <title>What does 95% mean?</title>
      
      <p>Recall that the goal of a confidence interval is to find a plausible range of values for a <em>parameter</em> of interest.</p>
      
      <p>The estimated statistic is not the value of interest, but it is typically the best guess for the unknown parameter.</p>
      
      <p>The confidence level (often 95%) is a number that takes a while to get used to.</p>
      
      <p>Surprisingly, the percentage does not describe the dataset at hand, it describes many possible datasets.</p>
      
      <p>One way to understand a confidence interval is to think about all the confidence intervals that you have ever made or that you will ever make as a scientist, the confidence level describes <alert>those</alert> intervals.</p>
      
      <p><xref ref="fig-ci25ints" /> demonstrates a hypothetical situation in which 25 different studies are performed on the exact same population (with the same goal of estimating the true parameter value of <m>p_1 - p_2 = 0.47).</m> The study at hand represents one point estimate (a dot) and a corresponding interval.</p>
      
      <p>It is not possible to know whether the interval at hand is to the right of the unknown true parameter value (the black line) or to the left of that line.</p>
      
      <p>It is also impossible to know whether the interval captures the true parameter (is blue) or does not (is red).</p>
      
      <p>If we are making 95% intervals, then about 5% of the intervals we create over our lifetime will <em>not</em> capture the parameter of interest (e.g., will be red as in <xref ref="fig-ci25ints" />).</p>
      
      <p>What we know is that over our lifetimes as scientists, about 95% of the intervals created and reported on will capture the parameter value of interest: thus the language "95% confident."</p>
      
      <figure xml:id="fig-ci25ints">
        <caption>One hypothetical population, parameter value of: <m>p_1 - p_2 = 0.47.</m> Twenty-five different studies all which led to a different point estimate, SE, and confidence interval. The study at hand is one of the horizontal lines (hopefully a blue line!).' out-width: 85% fig-alt: | A series of 25 horizontal lines are drawn, representing each of 25 different studies (where a study represents two samples, one from each of population 1 and population 2). Each vertical line starts at the value of the lower bound of the confidence interval and ends at the value of the upper bound of the confidence interval which was created from that particular sample. In the center of the line is a solid dot at the observed difference in proportion of successes for sample 1 minus sample 2. A dashed vertical line runs through the horizontal lines at p = 0.47 (which is the true value of the diffrence in population proportions). 24 of the 25 horizontal lines cross the vertical line at 0.47, but one of the horizontal lines is completely above than 0.47. The line that does not cross 0.47 is colored red because the confidence interval from that particular sample would not have captured the true difference in population proportions. fig-asp: 0.55</caption>
        <image source="images/fig-ci25ints-1.png" width="70%" />
      </figure>
      
      <p>The choice of 95% or 90% or even 99% as a confidence level is admittedly somewhat arbitrary; however, it is related to the logic we used when deciding that a p-value should be declared as "discernible" if it is lower than 0.05 (or 0.10 or 0.01, respectively).</p>
      
      <p>Indeed, one can show mathematically, that a 95% confidence interval and a two-sided hypothesis test at a cutoff of 0.05 will provide the same conclusion when the same data and mathematical tools are applied for the analysis.</p>
      
      <p>A full derivation of the explicit connection between confidence intervals and hypothesis tests is beyond the scope of this text.</p>
      
    </subsection>
    
  </section>
  
  <section xml:id="sec-math-2prop">
    <title>Mathematical model for the difference in proportions</title>
    
    <subsection>
      <title>Variability of the difference between two proportions</title>
      
      <p>Like with <m>\hat{p},</m> the difference of two sample proportions <m>\hat{p}_1 - \hat{p}_2</m> can be modeled using a normal distribution when certain conditions are met.</p>
      
      <p>First, we require a broader independence condition, and secondly, the success-failure condition must be met by both groups.</p>
      
      <p><alert>Conditions for the sampling distribution of</alert> <m>\hat{p}_1 -\hat{p}_2</m> <alert>to be normal.</alert></p>
      
      <p>The difference <m>\hat{p}_1 - \hat{p}_2</m> can be modeled using a normal distribution when</p>
      
      <ol>
        <li><p><em>Independence</em> (extended). The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment.</p></li>
        <li><p><em>Success-failure condition.</em> The success-failure condition holds for both groups, where we check successes and failures in each group separately. That is, we should have at least 10 successes and 10 failures in each of the two groups.</p></li>
      </ol>
      
      <p>When these conditions are satisfied, the standard error of <m>\hat{p}_1 - \hat{p}_2</m> is:</p>
      
      <p><me>SE(\hat{p}_1 - \hat{p}_2) = \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
      </me></p>
      <p>where <m>p_1</m> and <m>p_2</m> represent the population proportions, and <m>n_1</m> and <m>n_2</m> represent the sample sizes.</p>
      
      <p>Note that in most cases, the standard error is approximated using the observed data:</p>
      
      <p><me>SE(\hat{p}_1 - \hat{p}_2) = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}
      </me></p>
      <p>where <m>\hat{p}_1</m> and <m>\hat{p}_2</m> represent the observed sample proportions, and <m>n_1</m> and <m>n_2</m> represent the sample sizes.</p>
      
      <p>Recall that the margin of error is defined by the standard error.</p>
      
      <p>The margin of error for <m>\hat{p}_1 - \hat{p}_2</m> can be directly obtained from <m>SE(\hat{p}_1 - \hat{p}_2).</m></p>
      
      <p><alert>Margin of error for</alert> <m>\hat{p}_1 - \hat{p}_2.</m></p>
      
      <p>The margin of error is <m>z^\star \times \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}</m> where <m>z^\star</m> is calculated from a specified percentile on the normal distribution.</p>
      
      <p><idx><h>standard error (SE)</h><h>difference in proportions</h></idx></p>
      
      <p><idx><h>difference in proportions</h><h>standard error (SE)</h></idx></p>
      
    </subsection>
    
    <subsection>
      <title>Confidence interval for the difference between two proportions</title>
      
      <p><idx><h>Z-CI</h><h>difference in proportions</h></idx></p>
      
      <p>We can apply the generic confidence interval formula for a difference of two proportions, where we use <m>\hat{p}_1 - \hat{p}_2</m> as the point estimate and substitute the <m>SE</m> formula:</p>
      
      <md>
        <mrow>\text{point estimate} \amp \pm z^{\star} \times SE</mrow>
        <mrow>(\hat{p}_1 - \hat{p}_2) \amp \pm z^{\star} \times \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}</mrow>
      </md>
            
      <p>
        <alert>Standard error of the difference in two proportions,</alert> <m>\hat{p}_1 -\hat{p}_2.</m>
        When the conditions for the normal model are are met, the <alert>variability</alert> of the difference in proportions, <m>\hat{p}_1 -\hat{p}_2,</m> is well described by:
      </p>
      <me>SE(\hat{p}_1 -\hat{p}_2) = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}</me>
        <example>
        <statement>
        We reconsider the experiment for patients who underwent cardiopulmonary resuscitation (CPR) for a heart attack and were subsequently admitted to a hospital.
        These patients were randomly divided into a treatment group where they received a blood thinner or the control group where they did not receive a blood thinner.
        The outcome variable of interest was whether the patients survived for at least 24 hours.
        The results are shown in <xref ref="tbl-cpr-summary" />.
        Check whether we can model the difference in sample proportions using the normal distribution.
        </statement>
        <solution>
        We first check for independence: since this is a randomized experiment, it seems reasonable to assume that the observations are idependent.
        Next, we check the success-failure condition for each group.
        We have at least 10 successes and 10 failures in each experiment arm (11, 14, 39, 26), so this condition is also satisfied.
        With both conditions satisfied, the difference in sample proportions can be reasonably modeled using a normal distribution for these data.
        </solution>
        </example>
        <example>
        <statement>
        Create and interpret a 90% confidence interval of the difference for the survival rates in the CPR study.
        </statement>
        <solution>
          <p>
            We'll use <m>p_T</m> for the survival rate in the treatment group and <m>p_C</m> for the control group:
          </p>
          <me>\hat{p}_{T} - \hat{p}_{C} = \frac{14}{40} - \frac{11}{50}  = 0.35 - 0.22 = 0.13</me>
          <p>
            As with the one-sample proportion case, we use the sample estimates of each proportion in the formula in the confidence interval context:
          </p>
          <me>SE \approx \sqrt{\frac{0.35 (1 - 0.35)}{40} + \frac{0.22 (1 - 0.22)}{50}}  = 0.095</me>
          <p>
            For a 90% confidence interval, we use <m>z^{\star} = 1.65</m>:
          </p>
          <md>
            <mrow>\text{point estimate} \amp \pm z^{\star} \times SE</mrow>
            <mrow>0.13 \amp \pm 1.65 \times 0.095</mrow>
            <mrow>(-0.027 \amp , 0.287)</mrow>
          </md>
          <p>
            We are 90% confident that individuals receiving blood thinners have between a 2.7% less chance of survival to a 28.7% greater chance of survival than those in the control group.
            Because 0% is contained in the interval, we do not have enough information to say whether blood thinners help or harm heart attack patients who have been admitted after they have undergone CPR.
            Note, the problem was set up as 90% to indicate that there was not a need for a high level of confidence (such a 95% or 99%).
            A lower degree of confidence increases potential for error, but it also produces a more narrow interval.
          </p>
        </solution>
        </example>
        <exercise>
        <statement>
        A 5-year experiment was conducted to evaluate the effectiveness of fish oils on reducing cardiovascular events, where each subject was randomized into one of two treatment groups <xref ref="biblio-Manson-2019" />.
        We'll consider heart attack outcomes in the patients listed in <xref ref="tbl-fish-oil-data" />.
        Create a 95% confidence interval for the effect of fish oils on heart attacks for patients who are well-represented by those in the study.
        Also interpret the interval in the context of the study.
        </statement>
        <solution>
        Because the patients were randomized, the subjects are independent, both within and between the two groups.
        The success-failure condition is also met for both groups as all counts are at least 10.
        This satisfies the conditions necessary to model the difference in proportions using a normal distribution.
        Compute the sample proportions <m>(\hat{p}_{\text{fish oil}} = 0.0112,</m> <m>\hat{p}_{\text{placebo}} = 0.0155),</m> point estimate of the difference <m>(0.0112 - 0.0155 = -0.0043),</m> and standard error <m>SE = \sqrt{\frac{0.0112 \times 0.9888}{12933} + \frac{0.0155 \times 0.9845}{12938}},</m> <m>SE = 0.00145.</m> Next, plug the values into the general formula for a confidence interval, where we'll use a 95% confidence level with <m>z^{\star} = 1.96:</m> <m>-0.0043 \pm 1.96 \times 0.00145 = (-0.0071, -0.0015).</m> We are 95% confident that fish oils decreases heart attacks by 0.15 to 0.71 percentage points (off of a baseline of about 1.55%) over a 5-year period for subjects who are similar to those in the study.
        Because the interval is entirely below 0, and the treatment was randomly assigned the data provide strong evidence that fish oil supplements reduce heart attacks in patients like those in the study.
        </solution>
        </exercise>
        <table xml:id="tbl-fish-oil-data">
        <title>Results for the study on n-3 fatty acid supplement and related health benefits. tbl-pos: H</title>
        <tabular>
        <row><cell>Table content</cell></row>
        </tabular>
        </table>
        <note>
        <title>Data</title>
        The <url href="http://openintrostat.github.io/openintro/reference/fish_oil.html"><c>fish_oil_18</c></url> data can be found in the <url href="http://openintrostat.github.io/openintro"><alert>openintro</alert></url> R package.
        </note>
        </subsection>
<subsection xml:id="sec-test-prop-pool">
<title>Hypothesis test for the difference between two proportions</title>

The details for calculating a SE and for checking technical conditions are very similar to that of confidence intervals.
However, when the null hypothesis is that <m>p_1 - p_2 = 0,</m> we use a special proportion called the <alert>pooled proportion</alert><idx>pooled proportion</idx> to estimate the SE and to check the success-failure condition.

<alert>Use the pooled proportion when <m>H_0</m> is <m>p_1 - p_2 = 0.</m></alert>

When the null hypothesis is that the proportions are equal, use the pooled proportion <m>(\hat{p}_{\textit{pool}})</m> of successes to verify the success-failure condition and estimate the standard error:

<me>\hat{p}_{\textit{pool}} = \frac{\text{number of successes}}{\text{number of cases}} = \frac{\hat{p}_1 n_1 + \hat{p}_2 n_2}{n_1 + n_2}</me>

Here <m>\hat{p}_1 n_1</m> represents the number of successes in sample 1 because <m>\hat{p}_1 = \frac{\text{number of successes in sample 1}}{n_1}.</m>

Similarly, <m>\hat{p}_2 n_2</m> represents the number of successes in sample 2.

<alert>The test statistic for assessing two proportions is a Z.</alert>

The Z score<idx><h>Z score</h><h>difference in proportions</h></idx><idx><h>difference in proportions</h><h>Z score</h></idx> is a ratio of how the two sample proportions differ as compared to the expected variability of difference between the proportions.

<me>Z = \frac{(\hat{p}_1 - \hat{p}_2) - 0}{\sqrt{\hat{p}_{pool}(1-\hat{p}_{pool}) \bigg(\frac{1}{n_1} + \frac{1}{n_2} \bigg)}}</me>

When the null hypothesis is true and the conditions are met, Z has a standard normal distribution.
See the box below for calculation of the pooled proportion of successes.

Conditions:

<ul>
<li><p>Independent observations</p></li>
<li><p>Large samples: <m>(n_1 p_1 \geq 10</m> and <m>n_1 (1-p_1) \geq 10</m> and <m>n_2 p_2 \geq 10</m> and <m>n_2 (1-p_2) \geq 10)</m></p></li>
<li><p>Check conditions using: <m>(n_1 \hat{p}_{\textit{pool}} \geq 10</m> and <m>n_1 (1-\hat{p}_{\textit{pool}}) \geq 10</m> and <m>n_2 \hat{p}_{\textit{pool}}\geq 10</m> and <m>n_2 (1-\hat{p}_{\textit{pool}}) \geq 10)</m></p></li>
</ul>

A mammogram is an X-ray procedure used to check for breast cancer.
Whether mammograms should be used is part of a controversial discussion, and it's the topic of our next example where we learn about 2-proportion hypothesis tests when <m>H_0</m> is <m>p_1 - p_2 = 0</m> (or equivalently, <m>p_1 = p_2).</m>

A 30-year study was conducted with nearly 90,000 participants who identified as female.
During a 5-year screening period, each participant was randomized to one of two groups: in the first group, participants received regular mammograms to screen for breast cancer, and in the second group, participants received regular non-mammogram breast cancer exams.
No intervention was made during the following 25 years of the study, and we'll consider death resulting from breast cancer over the full 30-year period.
Results from the study are summarized in <xref ref="tbl-mammogramStudySummaryTable"/>.

<note>
<title>Data</title>
<p>The <url href="http://openintrostat.github.io/openintro/reference/mammogram.html"><c>mammogram</c></url> data can be found in the <url href="http://openintrostat.github.io/openintro"><alert>openintro</alert></url> R package.</p>
</note>

If mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group.
On the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see an increase in breast cancer deaths in the mammogram group.

<table xml:id="tbl-mammogramStudySummaryTable">
<title>Summary results for breast cancer study.</title>
<tabular>
<row><cell>Treatment</cell><cell colspan="2">Death from breast cancer?</cell></row>
<row><cell></cell><cell>Yes</cell><cell>No</cell></row>
<row><cell>Mammogram</cell><cell>500</cell><cell>44,425</cell></row>
<row><cell>Control</cell><cell>505</cell><cell>44,405</cell></row>
</tabular>
</table>

<exercise>
<statement>
<p>Is this study an experiment or an observational study?</p>
</statement>
<solution>
<p>This is an experiment.
Patients were randomized to receive mammograms or a standard breast cancer exam.
We will be able to make causal conclusions based on this study.</p>
</solution>
</exercise>

<exercise>
<statement>
<p>Set up hypotheses to test whether there was a difference in breast cancer deaths in the mammogram and control groups.</p>
</statement>
<solution>
<p><m>H_0:</m> the breast cancer death rate for patients screened using mammograms is the same as the breast cancer death rate for patients in the control, <m>p_{MGM} - p_{C} = 0.</m> <m>H_A:</m> the breast cancer death rate for patients screened using mammograms is different than the breast cancer death rate for patients in the control, <m>p_{MGM} - p_{C} \neq 0.</m></p>
</solution>
</exercise>

The research question describing mammograms is set up to address specific hypotheses (in contrast to a confidence interval for a parameter).
In order to fully take advantage of the hypothesis testing structure, we assess the randomness under the condition that the null hypothesis is true (as we always do for hypothesis testing).
Using the data from <xref ref="tbl-mammogramStudySummaryTable"/>, we will check the conditions for using a normal distribution to analyze the results of the study using a hypothesis test.

<me>
\begin{aligned}
\hat{p}_{\textit{pool}}
    &amp;= \frac
        {\text{number of patients who died from breast cancer in the entire study}}
        {\text{number of patients in the entire study}} \\
    &amp;= \frac{500 + 505}{500 + \text{44,425} + 505 + \text{44,405}} \\
    &amp;= 0.0112
\end{aligned}
</me>

This proportion is an estimate of the breast cancer death rate across the entire study, and it's our best estimate of the proportions <m>p_{MGM}</m> and <m>p_{C}</m> <em>if the null hypothesis is true that</em> <m>p_{MGM} = p_{C}.</m> We will also use this pooled proportion when computing the standard error.

<example>
<statement>
<p>Is it reasonable to model the difference in proportions using a normal distribution in this study?</p>
</statement>
<solution>
<p>Because the patients were randomized, observations can be assumed to be independent, both within each group and between treatment groups.
We also must check the success-failure condition for each group.
Under the null hypothesis, the proportions <m>p_{MGM}</m> and <m>p_{C}</m> are equal, so we check the success-failure condition with our best estimate of these values under <m>H_0,</m> the pooled proportion from the two samples, <m>\hat{p}_{\textit{pool}} = 0.0112:</m></p>

<me>
\begin{aligned}
\hat{p}_{\textit{pool}} \times n_{MGM}
      &amp;= 0.0112 \times \text{44,925} = 503\\
 (1 - \hat{p}_{\textit{pool}}) \times n_{MGM}
      &amp;= 0.9888 \times \text{44,925} = \text{44,422} \\
  \hat{p}_{\textit{pool}} \times n_{C}
      &amp;= 0.0112 \times \text{44,910} = 503\\
  (1 - \hat{p}_{\textit{pool}}) \times n_{C}
      &amp;= 0.9888 \times \text{44,910} = \text{44,407}
\end{aligned}
</me>

<p>The success-failure condition is satisfied since all values are at least 10.
With both conditions satisfied, we can safely model the difference in proportions using a normal distribution.</p>
</solution>
</example>

In the previous example, the pooled proportion was used to check the success-failure condition.
In the next example, we see an additional place where the pooled proportion comes into play: the standard error calculation.

<example>
<statement>
<p>Compute the point estimate of the difference in breast cancer death rates in the two groups, and use the pooled proportion <m>\hat{p}_{\textit{pool}} = 0.0112</m> to calculate the standard error.</p>
</statement>
<solution>
<p>The point estimate of the difference in breast cancer death rates is</p>

<me>
\hat{p}_{MGM} - \hat{p}_{C} = \frac{500}{500 + 44,425} - \frac{505}{505 + 44,405} = 0.01113 - 0.01125 = -0.00012
</me>

<p>The breast cancer death rate in the mammogram group was 0.012% less than in the control group.
Next, the standard error is calculated <em>using the pooled proportion,</em> <m>\hat{p}_{\textit{pool}}:</m></p>

<me>SE = \sqrt{\frac{\hat{p}_{\textit{pool}}(1-\hat{p}_{\textit{pool}})}{n_{MGM}} + \frac{\hat{p}_{\textit{pool}}(1-\hat{p}_{\textit{pool}})}{n_{C}}}= 0.00070</me>
</solution>
</example>

<example>
<statement>
<p>Using the point estimate <m>\hat{p}_{MGM} - \hat{p}_{C} = -0.00012</m> and standard error <m>SE = 0.00070,</m> calculate a p-value for the hypothesis test and write a conclusion.</p>
</statement>
<solution>
<p>We first compute a test statistic and draw a picture:</p>

<me>Z = \frac{\text{point estimate} - \text{null value}}{SE} = \frac{-0.00012 - 0}{0.00070} = -0.17</me>

<!-- R plot of normal distribution with shaded tails would go here -->

<p>The lower tail area is 0.4325, which we double to get the p-value: 0.8650.
Because this p-value is larger than 0.05, we do not reject the null hypothesis.
That is, the difference in breast cancer death rates is likely to have occurred just by chance, if the null hypothesis is true.
Thus, we do not observe benefits or harm from mammograms relative to a regular breast exam.</p>
</solution>
</example>

Can we conclude that mammograms have no benefits or harm?
Here are a few considerations to keep in mind when reviewing the mammogram study as well as any other medical study:

<ul>
<li><p>We do not reject the null hypothesis, which means we do not have sufficient evidence to conclude that mammograms reduce or increase breast cancer deaths.</p></li>
<li><p>If mammograms are helpful or harmful, the data suggest the effect isn't very large.</p></li>
<li><p>Are mammograms more or less expensive than a non-mammogram breast exam? If one option is much more expensive than the other and does not offer clear benefits, then we should lean towards the less expensive option.</p></li>
<li><p>The study's authors also found that mammograms led to over-diagnosis of breast cancer, which means some breast cancers were found (or thought to be found) but that these cancers would not cause symptoms during patients' lifetimes. That is, something else would kill the patient before breast cancer symptoms appeared. This means some patients may have been treated for breast cancer unnecessarily, and this treatment is another cost to consider. It is also important to recognize that over-diagnosis can cause unnecessary physical or emotional harm to patients.</p></li>
</ul>

These considerations highlight the complexity around medical care and treatment recommendations.
Experts and medical boards who study medical treatments use considerations like those above to provide their best recommendation based on the current evidence.

</subsection>

</section>

<section xml:id="sec-chp17-review">
<title>Chapter review</title>

<subsection>
<title>Summary</title>

<p>When the parameter of interest is the difference in population proportions across two groups, randomization tests, bootstrapping, and mathematical modeling can be applied.
For confidence intervals, bootstrapping from each group separately will provide a sampling distribution for the difference in sample proportions; the mathematical model shows a similar distributional shape as long as the sample size is large enough to fulfill the success-failure conditions and so that the data are representative of the entire population.
Keep in mind that some datasets will produce a confidence interval which does not capture the true parameter, this is the nature of variability!
Over your lifetime, about 95% of the confidence intervals you create will capture the parameter of interest, and about 5% won't.
For hypothesis testing, repeated randomization of the explanatory variable creates a null distribution of differences in sample proportions that could have occurred under the null hypothesis.
Randomization and the mathematical model will have similar null distributions, as long as the sample size is large enough to fulfill the success-failure conditions.</p>

</subsection>

<subsection>
<title>Terms</title>

<p>The terms introduced in this chapter are presented in <xref ref="tbl-terms-chp-17"/>.
If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
You should be able to easily spot them as <alert>bolded text</alert>.</p>

<table xml:id="tbl-terms-chp-17">
<title>Terms introduced in this chapter.</title>
<tabular>
<!-- Terms table content generated from R would go here -->
</tabular>
</table>

</subsection>

</section>

<section xml:id="sec-chp17-exercises">
<title>Exercises</title>

<p>Answers to odd-numbered exercises can be found in <xref ref="sec-exercise-solutions-17"/>.</p>

<!-- Exercises from exercises/_17-ex-inference-two-props.qmd would be included here -->

</section>

</chapter>